
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>33. Classical Prediction and Filtering With Linear Algebra &#8212; Advanced Quantitative Economics with Python</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" href="_static/styles/quantecon-book-theme.css?digest=3bd9fcddbe64f63e07c8604843d1cc622a07b430" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css?v=982b99e0" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=b4b7a797" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>


    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script src="_static/scripts/quantecon-book-theme.js?digest=30f7c850c5b005eca2ad6e48893cd350a6c6c4c2"></script>
    <script src="_static/scripts/jquery.js?v=5d32c60e"></script>
    <script src="_static/scripts/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-KZLV7PM9LL"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-KZLV7PM9LL');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-KZLV7PM9LL');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min", "col": "col", "Span": "span", "epsilon": "\\varepsilon", "EE": "\\mathbb{E}", "PP": "\\mathbb{P}", "RR": "\\mathbb{R}", "NN": "\\mathbb{N}", "ZZ": "\\mathbb{Z}", "aA": "\\mathcal{A}", "bB": "\\mathcal{B}", "cC": "\\mathcal{C}", "dD": "\\mathcal{D}", "eE": "\\mathcal{E}", "fF": "\\mathcal{F}", "gG": "\\mathcal{G}", "hH": "\\mathcal{H}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'classical_filtering';</script>
    <link rel="canonical" href="https://python-advanced.quantecon.org/classical_filtering.html" />
    <link rel="icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="34. Knowing the Forecasts of Others" href="knowing_forecasts_of_others.html" />
    <link rel="prev" title="32. Classical Control with Linear Algebra" href="lu_tricks.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on advanced quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Classical Prediction and Filtering With Linear Algebra"/>
<meta name="twitter:description" content="This website presents a set of lectures on advanced quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Classical Prediction and Filtering With Linear Algebra" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python-advanced.quantecon.org/classical_filtering.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on advanced quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Advanced Quantitative Economics with Python" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>

<!-- Override QuantEcon theme colors -->

    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=classical_filtering>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">33.1. Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#references">33.1.1. References</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-dimensional-prediction">33.2. Finite dimensional prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation">33.2.1. Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1">33.2.2. Example 1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2">33.2.3. Example 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction">33.2.4. Prediction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#combined-finite-dimensional-control-and-prediction">33.3. Combined finite dimensional control and prediction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-prediction-and-filtering-problems">33.4. Infinite horizon prediction and filtering problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-formulation">33.4.1. Problem formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">33.5. Exercises</a></li>
</ul>
                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo logo-img" alt="logo"></a>
                                    
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/en/stable/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="intro.html">Advanced Quantitative Economics with Python</a></p>

                        <p class="qe-page__header-subheading">Classical Prediction and Filtering With Linear Algebra</p>

                    </div>
                    <!-- length 2, since its a string and empty dict has length 2 - {} -->
                        <p class="qe-page__header-authors" font-size="18">
                            
                                
                                    <a href="http://www.tomsargent.com/" target="_blank"><span>Thomas J. Sargent</span></a>
                                
                            
                                
                                    and <a href="https://johnstachurski.net/" target="_blank"><span>John Stachurski</span></a>
                                
                            
                        </p>


                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <div id="qe-notebook-header" align="right" style="text-align:right;">
        <a href="https://quantecon.org/" title="quantecon.org">
                <img style="width:250px;display:inline;" width="250px" src="https://assets.quantecon.org/img/qe-menubar-logo.svg" alt="QuantEcon">
        </a>
</div><section class="tex2jax_ignore mathjax_ignore" id="classical-prediction-and-filtering-with-linear-algebra">
<h1><span class="section-number">33. </span>Classical Prediction and Filtering With Linear Algebra<a class="headerlink" href="#classical-prediction-and-filtering-with-linear-algebra" title="Link to this heading">#</a></h1>
<section id="overview">
<h2><span class="section-number">33.1. </span>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>This is a sequel to the earlier lecture <a class="reference internal" href="lu_tricks.html"><span class="doc">Classical Control with Linear Algebra</span></a>.</p>
<p>That lecture used linear algebra – in particular,  the <a class="reference external" href="https://en.wikipedia.org/wiki/LU_decomposition">LU decomposition</a>  – to formulate and solve a class of linear-quadratic optimal control problems.</p>
<p>In this lecture, we’ll be using a closely related decomposition,
the <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition">Cholesky decomposition</a>, to solve linear prediction and filtering problems.</p>
<p>We exploit the useful fact that there is an intimate connection between two superficially different classes of problems:</p>
<ul class="simple">
<li><p>deterministic linear-quadratic (LQ) optimal control problems</p></li>
<li><p>linear least squares prediction and filtering problems</p></li>
</ul>
<p>The first class of problems involves no randomness, while the second is all about randomness.</p>
<p>Nevertheless,  essentially the same mathematics  solves both types of problem.</p>
<p>This connection, which is often termed “duality,” is present whether one uses “classical” or “recursive” solution procedures.</p>
<p>In fact, we saw duality at work earlier when we formulated control and prediction problems recursively in lectures <a class="reference external" href="https://python-intro.quantecon.org/lqcontrol.html">LQ dynamic programming problems</a>, <a class="reference external" href="https://python-intro.quantecon.org/kalman.html">A first look at the Kalman filter</a>, and <a class="reference external" href="https://python-intro.quantecon.org/perm_income.html">The permanent income model</a>.</p>
<p>A useful consequence of duality is that</p>
<ul class="simple">
<li><p>With every LQ control problem, there is implicitly affiliated a linear least squares prediction or filtering problem.</p></li>
<li><p>With every linear least squares prediction or filtering problem there is implicitly affiliated a LQ control problem.</p></li>
</ul>
<p>An understanding of these connections has repeatedly proved useful in cracking interesting applied problems.</p>
<p>For example, Sargent <span id="id1">[<a class="reference internal" href="zreferences.html#id200" title="Thomas J Sargent. Macroeconomic Theory. Academic Press, New York, 2nd edition, 1987.">Sargent, 1987</a>]</span> [chs. IX, XIV] and Hansen and Sargent <span id="id2">[<a class="reference internal" href="zreferences.html#id110" title="Lars Peter Hansen and Thomas J Sargent. Formulating and estimating dynamic linear rational expectations models. Journal of Economic Dynamics and control, 2:7–46, 1980.">Hansen and Sargent, 1980</a>]</span> formulated
and solved control and filtering problems using <span class="math notranslate nohighlight">\(z\)</span>-transform methods.</p>
<p>In this lecture, we begin to investigate these ideas by using mostly elementary linear algebra.</p>
<p>This is the main purpose and focus of the lecture.</p>
<p>However, after showing matrix algebra formulas, we’ll summarize classic infinite-horizon formulas built on <span class="math notranslate nohighlight">\(z\)</span>-transform  and lag
operator methods.</p>
<p>And we’ll occasionally refer to some of these formulas from the infinite dimensional problems as we present the finite time
formulas and associated linear algebra.</p>
<p>We’ll start with the following standard import:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<section id="references">
<h3><span class="section-number">33.1.1. </span>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h3>
<p>Useful references include <span id="id3">[<a class="reference internal" href="zreferences.html#id109" title="Peter Whittle. Prediction and regulation by linear least-square methods. English Univ. Press, 1963.">Whittle, 1963</a>]</span>, <span id="id4">[<a class="reference internal" href="zreferences.html#id110" title="Lars Peter Hansen and Thomas J Sargent. Formulating and estimating dynamic linear rational expectations models. Journal of Economic Dynamics and control, 2:7–46, 1980.">Hansen and Sargent, 1980</a>]</span>, <span id="id5">[<a class="reference internal" href="zreferences.html#id111" title="Sophocles J Orfanidis. Optimum Signal Processing: An Introduction. McGraw Hill Publishing, New York, New York, 1988.">Orfanidis, 1988</a>]</span>, <span id="id6">[<a class="reference internal" href="zreferences.html#id112" title="Papoulis Athanasios and S Unnikrishna Pillai. Probability, random variables, and stochastic processes. Mc-Graw Hill, 1991.">Athanasios and Pillai, 1991</a>]</span>, and <span id="id7">[<a class="reference internal" href="zreferences.html#id113" title="John F Muth. Optimal properties of exponentially weighted forecasts. Journal of the american statistical association, 55(290):299–306, 1960.">Muth, 1960</a>]</span>.</p>
</section>
</section>
<section id="finite-dimensional-prediction">
<h2><span class="section-number">33.2. </span>Finite dimensional prediction<a class="headerlink" href="#finite-dimensional-prediction" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\((x_1, x_2, \ldots, x_T)^\prime = x\)</span> be a <span class="math notranslate nohighlight">\(T \times 1\)</span> vector of random variables with mean <span class="math notranslate nohighlight">\(\mathbb{E} x = 0\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\mathbb{E} xx^\prime = V\)</span>.</p>
<p>Here <span class="math notranslate nohighlight">\(V\)</span> is a <span class="math notranslate nohighlight">\(T \times T\)</span> positive definite matrix.</p>
<p>The <span class="math notranslate nohighlight">\(i,j\)</span> component <span class="math notranslate nohighlight">\(E x_i x_j\)</span> of <span class="math notranslate nohighlight">\(V\)</span> is the <strong>inner product</strong>   between <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span>.</p>
<p>We regard the random variables as being
ordered in time so that <span class="math notranslate nohighlight">\(x_t\)</span> is thought of as the value of some
economic variable at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>For example, <span class="math notranslate nohighlight">\(x_t\)</span> could be generated by the random process described  by the Wold representation presented in equation <a class="reference internal" href="#equation-eq-31">(33.16)</a>
in the section below on infinite dimensional prediction and filtering.</p>
<p>In that case, <span class="math notranslate nohighlight">\(V_{ij}\)</span> is given by the coefficient on <span class="math notranslate nohighlight">\(z^{\mid i-j \mid}\)</span> in the expansion of <span class="math notranslate nohighlight">\(g_x (z) = d(z) \, d(z^{-1}) + h\)</span>, which equals
<span class="math notranslate nohighlight">\(h+\sum^\infty_{k=0} d_k d_{k+\mid i-j \mid}\)</span>.</p>
<p>We want to  construct <span class="math notranslate nohighlight">\(j\)</span> step ahead linear least squares predictors of the form</p>
<div class="math notranslate nohighlight">
\[
\mathbb{\hat E}
\left[
    x_T\vert x_{T-j}, x_{T-j + 1}, \ldots, x_1
\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{\hat E}\)</span> is the linear least squares projection operator.</p>
<p>(Sometimes <span class="math notranslate nohighlight">\(\mathbb{\hat E}\)</span> is called the wide-sense expectations operator)</p>
<p>To find linear least squares predictors it is helpful  first to construct a <span class="math notranslate nohighlight">\(T \times 1\)</span> vector <span class="math notranslate nohighlight">\(\varepsilon\)</span>
of random variables that form an orthonormal basis   for the vector of random variables <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>The key insight here comes from noting that because the covariance matrix <span class="math notranslate nohighlight">\(V\)</span> of <span class="math notranslate nohighlight">\(x\)</span> is a positive definite and symmetric,
there exists a (Cholesky) decomposition of <span class="math notranslate nohighlight">\(V\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
V = L^{-1} (L^{-1})^\prime
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
L \, V \, L^\prime = I
\]</div>
<p>where <span class="math notranslate nohighlight">\(L\)</span>  and <span class="math notranslate nohighlight">\(L^{-1}\)</span> are both lower triangular.</p>
<p>Form the <span class="math notranslate nohighlight">\(T \times 1\)</span> random vector <span class="math notranslate nohighlight">\(\varepsilon = Lx\)</span>.</p>
<p>The random vector <span class="math notranslate nohighlight">\(\varepsilon\)</span> is an orthonormal basis for <span class="math notranslate nohighlight">\(x\)</span> because</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L\)</span> is nonsingular</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E} \, \varepsilon \, \varepsilon^\prime = L \mathbb{E} xx^\prime L^\prime = I\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x = L^{-1} \varepsilon\)</span></p></li>
</ul>
<p>It is enlightening  to write out and interpret the equations <span class="math notranslate nohighlight">\(Lx = \varepsilon\)</span> and <span class="math notranslate nohighlight">\(L^{-1} \varepsilon = x\)</span>.</p>
<p>First, we’ll write <span class="math notranslate nohighlight">\(Lx = \varepsilon\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-53">
<span class="eqno">(33.1)<a class="headerlink" href="#equation-eq-53" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    L_{11} x_1 &amp;= \varepsilon_1 \\
    L_{21}x_1 + L_{22} x_2 &amp;= \varepsilon_2 \\ \, \vdots \\
    L_{T1} \, x_1 \, \ldots \, + L_{TT} x_T &amp;= \varepsilon_T
\end{aligned}\end{split}\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-54">
<span class="eqno">(33.2)<a class="headerlink" href="#equation-eq-54" title="Link to this equation">#</a></span>\[\sum^{t-1}_{j=0} L_{t,t-j}\, x_{t-j} = \varepsilon_t, \quad t = 1, \, 2, \ldots T\]</div>
<p>Next, we write <span class="math notranslate nohighlight">\(L^{-1} \varepsilon = x\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-55a">
<span class="eqno">(33.3)<a class="headerlink" href="#equation-eq-55a" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
     x_1 &amp;= L_{11}^{-1} \varepsilon_1 \\
     x_2 &amp;= L_{22}^{-1} \varepsilon_2 + L_{21}^{-1} \varepsilon_1  \\ \, \vdots \\
     x_T &amp;=  L_{TT}^{-1} \varepsilon_T + L_{T,T-1}^{-1} \varepsilon_{T-1} \, \ldots \, + L_{T,1}^{-1} \varepsilon_1
\end{aligned},\end{split}\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-55">
<span class="eqno">(33.4)<a class="headerlink" href="#equation-eq-55" title="Link to this equation">#</a></span>\[x_t = \sum^{t-1}_{j=0} L^{-1}_{t,t-j}\, \varepsilon_{t-j}\]</div>
<p>where <span class="math notranslate nohighlight">\(L^{-1}_{i,j}\)</span> denotes the <span class="math notranslate nohighlight">\(i,j\)</span> element of <span class="math notranslate nohighlight">\(L^{-1}\)</span>.</p>
<p>From <a class="reference internal" href="#equation-eq-54">(33.2)</a>, it follows that <span class="math notranslate nohighlight">\(\varepsilon_t\)</span> is in the linear subspace spanned by <span class="math notranslate nohighlight">\(x_t,\, x_{t-1}, \ldots,\, x_1\)</span>.</p>
<p>From <a class="reference internal" href="#equation-eq-55">(33.4)</a> it follows that  that <span class="math notranslate nohighlight">\(x_t\)</span> is in the linear subspace spanned by
<span class="math notranslate nohighlight">\(\varepsilon_t, \, \varepsilon_{t-1}, \ldots, \varepsilon_1\)</span>.</p>
<p>Equation <a class="reference internal" href="#equation-eq-54">(33.2)</a> forms  a sequence of <strong>autoregressions</strong>  that for <span class="math notranslate nohighlight">\(t = 1, \ldots, T\)</span> express
<span class="math notranslate nohighlight">\(x_t\)</span>  as linear functions of <span class="math notranslate nohighlight">\(x_s, s = 1, \ldots, t-1\)</span> and a random variable <span class="math notranslate nohighlight">\((L_{t,t})^{-1} \varepsilon_t\)</span>
that is orthogonal to each componenent of <span class="math notranslate nohighlight">\(x_s, s = 1, \ldots, t-1\)</span>.</p>
<p>(Here <span class="math notranslate nohighlight">\((L_{t,t})^{-1}\)</span> denotes the reciprocal of <span class="math notranslate nohighlight">\(L_{t,t}\)</span> while <span class="math notranslate nohighlight">\(L_{t,t}^{-1}\)</span> denotes the <span class="math notranslate nohighlight">\(t,t\)</span> element
of <span class="math notranslate nohighlight">\(L^{-1}\)</span>).</p>
<p>The equivalence of the subspaces spanned by <span class="math notranslate nohighlight">\(\varepsilon_t, \ldots, \varepsilon_1\)</span> and <span class="math notranslate nohighlight">\(x_t, \ldots, x_1\)</span> means that
for <span class="math notranslate nohighlight">\(t-1\geq m \geq 1\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-56">
<span class="eqno">(33.5)<a class="headerlink" href="#equation-eq-56" title="Link to this equation">#</a></span>\[\mathbb{\hat E}
[ x_t \mid x_{t-m},\, x_{t-m-1}, \ldots, x_1 ] =
\mathbb{\hat E}
[x_t \mid \varepsilon_{t-m}, \varepsilon_{t-m-1},\ldots, \varepsilon_1]\]</div>
<p>To proceed, it is useful to drill down and note that for <span class="math notranslate nohighlight">\(t-1 \geq m \geq 1\)</span> we can  rewrite <a class="reference internal" href="#equation-eq-55">(33.4)</a>  in the form of the
<strong>moving average representation</strong></p>
<div class="math notranslate nohighlight" id="equation-eq-57">
<span class="eqno">(33.6)<a class="headerlink" href="#equation-eq-57" title="Link to this equation">#</a></span>\[x_t = \sum^{m-1}_{j=0} L_{t,t-j}^{-1}\, \varepsilon_{t-j} + \sum^{t-1}_{j=m}
L^{-1}_{t, t-j}\, \varepsilon_{t-j}\]</div>
<p>Representation <a class="reference internal" href="#equation-eq-57">(33.6)</a>  is an orthogonal decomposition of <span class="math notranslate nohighlight">\(x_t\)</span> into a part <span class="math notranslate nohighlight">\(\sum^{t-1}_{j=m} L_{t, t-j}^{-1}\, \varepsilon_{t-j}\)</span>
that lies in the space spanned by <span class="math notranslate nohighlight">\([x_{t-m},\, x_{t-m+1},\, \ldots, x_1]\)</span> and an orthogonal component
<span class="math notranslate nohighlight">\(\sum^{t-1}_{j=m} L^{-1}_{t, t-j}\, \varepsilon_{t-j}\)</span> that does not lie in that space but instead in a linear space knowns as its <strong>orthogonal complement</strong>.</p>
<p>It follows that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{\hat E}  [ x_t \mid x_{t-m},\, x_{t-m-1}, \ldots, x_1 ] =  \sum^{m-1}_{j=0} L_{t,t-j}^{-1}\, \varepsilon_{t-j}
\]</div>
<section id="implementation">
<h3><span class="section-number">33.2.1. </span>Implementation<a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<p>Here’s the code that computes solutions to LQ control and filtering problems using the methods described here and in <a class="reference internal" href="lu_tricks.html"><span class="doc">Classical Control with Linear Algebra</span></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">spst</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.linalg</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">la</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LQFilter</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y_m</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">h_eps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">β</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">            d : list or numpy.array (1-D or a 2-D column vector)</span>
<span class="sd">                    The order of the coefficients: [d_0, d_1, ..., d_m]</span>
<span class="sd">            h : scalar</span>
<span class="sd">                    Parameter of the objective function (corresponding to the</span>
<span class="sd">                    quadratic term)</span>
<span class="sd">            y_m : list or numpy.array (1-D or a 2-D column vector)</span>
<span class="sd">                    Initial conditions for y</span>
<span class="sd">            r : list or numpy.array (1-D or a 2-D column vector)</span>
<span class="sd">                    The order of the coefficients: [r_0, r_1, ..., r_k]</span>
<span class="sd">                    (optional, if not defined -&gt; deterministic problem)</span>
<span class="sd">            β : scalar</span>
<span class="sd">                    Discount factor (optional, default value is one)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">=</span> <span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y_m</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;y_m must be of length m = </span><span class="si">{self.m:d}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1">#---------------------------------------------</span>
        <span class="c1"># Define the coefficients of ϕ upfront</span>
        <span class="c1">#---------------------------------------------</span>
        <span class="n">ϕ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">ϕ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> \
                                           <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
                                           <span class="n">k</span><span class="o">=-</span><span class="n">i</span>
                                           <span class="p">)</span>
                                    <span class="p">)</span>
        <span class="n">ϕ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">ϕ</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ϕ</span> <span class="o">=</span> <span class="n">ϕ</span>

        <span class="c1">#-----------------------------------------------------</span>
        <span class="c1"># If r is given calculate the vector ϕ_r</span>
        <span class="c1">#-----------------------------------------------------</span>
        <span class="k">if</span> <span class="n">r</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">ϕ_r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">ϕ_r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> \
                                                 <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
                                                 <span class="n">k</span><span class="o">=-</span><span class="n">i</span>
                                                 <span class="p">)</span>
                                        <span class="p">)</span>
            <span class="k">if</span> <span class="n">h_eps</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ϕ_r</span> <span class="o">=</span> <span class="n">ϕ_r</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ϕ_r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">ϕ_r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">h_eps</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ϕ_r</span> <span class="o">=</span> <span class="n">ϕ_r</span>

        <span class="c1">#-----------------------------------------------------</span>
        <span class="c1"># If β is given, define the transformed variables</span>
        <span class="c1">#-----------------------------------------------------</span>
        <span class="k">if</span> <span class="n">β</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">β</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">β</span> <span class="o">=</span> <span class="n">β</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">β</span><span class="o">**</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">β</span><span class="o">**</span><span class="p">(</span><span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> \
                                   <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct_W_and_Wm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This constructs the matrices W and W_m for a given number of periods N</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span>
        <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span>

        <span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">W_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>

        <span class="c1">#---------------------------------------</span>
        <span class="c1"># Terminal conditions</span>
        <span class="c1">#---------------------------------------</span>

        <span class="n">D_m1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">M</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>

        <span class="c1"># (1) Constuct the D_{m+1} matrix using the formula</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">D_m1</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">d</span><span class="p">[:</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">@</span> <span class="n">d</span><span class="p">[</span><span class="n">k</span> <span class="o">-</span> <span class="n">j</span><span class="p">:</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Make the matrix symmetric</span>
        <span class="n">D_m1</span> <span class="o">=</span> <span class="n">D_m1</span> <span class="o">+</span> <span class="n">D_m1</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">D_m1</span><span class="p">))</span>

        <span class="c1"># (2) Construct the M matrix using the entries of D_m1</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">M</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">D_m1</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">]</span>

        <span class="c1">#----------------------------------------------</span>
        <span class="c1"># Euler equations for t = 0, 1, ..., N-(m+1)</span>
        <span class="c1">#----------------------------------------------</span>
        <span class="n">ϕ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ϕ</span>

        <span class="n">W</span><span class="p">[:(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="p">:(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">D_m1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">W</span><span class="p">[:(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">M</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">m</span><span class="p">)):</span>
            <span class="n">W</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">ϕ</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">W</span><span class="p">[</span><span class="n">N</span> <span class="o">-</span> <span class="n">m</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">):]</span> <span class="o">=</span> <span class="n">ϕ</span><span class="p">[:</span><span class="o">-</span><span class="n">i</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
            <span class="n">W_m</span><span class="p">[</span><span class="n">N</span> <span class="o">-</span> <span class="n">i</span><span class="p">,</span> <span class="p">:(</span><span class="n">m</span> <span class="o">-</span> <span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">ϕ</span><span class="p">[(</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">i</span><span class="p">):]</span>

        <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">W_m</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">roots_of_characteristic</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function calculates z_0 and the 2m roots of the characteristic</span>
<span class="sd">        equation associated with the Euler equation (1.7)</span>

<span class="sd">        Note:</span>
<span class="sd">        ------</span>
<span class="sd">        numpy.poly1d(roots, True) defines a polynomial using its roots that can</span>
<span class="sd">        be evaluated at any point. If x_1, x_2, ... , x_m are the roots then</span>
<span class="sd">            p(x) = (x - x_1)(x - x_2)...(x - x_m)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span>
        <span class="n">ϕ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ϕ</span>

        <span class="c1"># Calculate the roots of the 2m-polynomial</span>
        <span class="n">roots</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">roots</span><span class="p">(</span><span class="n">ϕ</span><span class="p">)</span>
        <span class="c1"># Sort the roots according to their length (in descending order)</span>
        <span class="n">roots_sorted</span> <span class="o">=</span> <span class="n">roots</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">roots</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>

        <span class="n">z_0</span> <span class="o">=</span> <span class="n">ϕ</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">roots</span><span class="p">,</span> <span class="kc">True</span><span class="p">)(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">z_1_to_m</span> <span class="o">=</span> <span class="n">roots_sorted</span><span class="p">[:</span><span class="n">m</span><span class="p">]</span>     <span class="c1"># We need only those outside the unit circle</span>

        <span class="n">λ</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">z_1_to_m</span>

        <span class="k">return</span> <span class="n">z_1_to_m</span><span class="p">,</span> <span class="n">z_0</span><span class="p">,</span> <span class="n">λ</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">coeffs_of_c</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        This function computes the coefficients {c_j, j = 0, 1, ..., m} for</span>
<span class="sd">                c(z) = sum_{j = 0}^{m} c_j z^j</span>

<span class="sd">        Based on the expression (1.9). The order is</span>
<span class="sd">            c_coeffs = [c_0, c_1, ..., c_{m-1}, c_m]</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">z_1_to_m</span><span class="p">,</span> <span class="n">z_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roots_of_characteristic</span><span class="p">()[:</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">c_0</span> <span class="o">=</span> <span class="p">(</span><span class="n">z_0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">z_1_to_m</span><span class="p">)</span><span class="o">.</span><span class="n">real</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mf">.5</span><span class="p">)</span>
        <span class="n">c_coeffs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">z_1_to_m</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">c</span> <span class="o">*</span> <span class="n">z_0</span> <span class="o">/</span> <span class="n">c_0</span>

        <span class="k">return</span> <span class="n">c_coeffs</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">solution</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function calculates {λ_j, j=1,...,m} and {A_j, j=1,...,m}</span>
<span class="sd">        of the expression (1.15)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">λ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">roots_of_characteristic</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">c_0</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coeffs_of_c</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">complex</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">):</span>
            <span class="n">denom</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">λ</span><span class="o">/</span><span class="n">λ</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
            <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">c_0</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">denom</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">)</span> <span class="o">!=</span> <span class="n">j</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">λ</span><span class="p">,</span> <span class="n">A</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">construct_V</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        This function constructs the covariance matrix for x^N (see section 6)</span>
<span class="sd">        for a given period N</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">ϕ_r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ϕ_r</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
                <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">j</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">:</span>
                    <span class="n">V</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">ϕ_r</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">j</span><span class="p">)]</span>

        <span class="k">return</span> <span class="n">V</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">simulate_a</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Assuming that the u&#39;s are normal, this method draws a random path</span>
<span class="sd">        for x^N</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">construct_V</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">d</span> <span class="o">=</span> <span class="n">spst</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">V</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">d</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a_hist</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This function implements the prediction formula discussed in section 6 (1.59)</span>
<span class="sd">        It takes a realization for a^N, and the period in which the prediction is </span>
<span class="sd">        formed</span>

<span class="sd">        Output:  E[abar | a_t, a_{t-1}, ..., a_1, a_0]</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a_hist</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">a_hist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a_hist</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">construct_V</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">aux_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">aux_matrix</span><span class="p">[:(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="p">:(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
        <span class="n">Ea_hist</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">L</span><span class="p">)</span> <span class="o">@</span> <span class="n">aux_matrix</span> <span class="o">@</span> <span class="n">L</span> <span class="o">@</span> <span class="n">a_hist</span>

        <span class="k">return</span> <span class="n">Ea_hist</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">optimal_y</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a_hist</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        - if t is NOT given it takes a_hist (list or numpy.array) as a</span>
<span class="sd">          deterministic a_t</span>
<span class="sd">        - if t is given, it solves the combined control prediction problem </span>
<span class="sd">          (section 7)(by default, t == None -&gt; deterministic)</span>

<span class="sd">        for a given sequence of a_t (either deterministic or a particular</span>
<span class="sd">        realization), it calculates the optimal y_t sequence using the method</span>
<span class="sd">        of the lecture</span>

<span class="sd">        Note:</span>
<span class="sd">        ------</span>
<span class="sd">        scipy.linalg.lu normalizes L, U so that L has unit diagonal elements</span>
<span class="sd">        To make things consistent with the lecture, we need an auxiliary</span>
<span class="sd">        diagonal matrix D which renormalizes L and U</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a_hist</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">W</span><span class="p">,</span> <span class="n">W_m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">construct_W_and_Wm</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

        <span class="n">L</span><span class="p">,</span> <span class="n">U</span> <span class="o">=</span> <span class="n">la</span><span class="o">.</span><span class="n">lu</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">permute_l</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">U</span><span class="p">))</span>
        <span class="n">U</span> <span class="o">=</span> <span class="n">D</span> <span class="o">@</span> <span class="n">U</span>
        <span class="n">L</span> <span class="o">=</span> <span class="n">L</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>

        <span class="n">J</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>   <span class="c1"># If the problem is deterministic</span>

            <span class="n">a_hist</span> <span class="o">=</span> <span class="n">J</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">a_hist</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1">#--------------------------------------------</span>
            <span class="c1"># Transform the &#39;a&#39; sequence if β is given</span>
            <span class="c1">#--------------------------------------------</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">β</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">a_hist</span> <span class="o">=</span>  <span class="n">a_hist</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">β</span><span class="o">**</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> \
                                    <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">a_bar</span> <span class="o">=</span> <span class="n">a_hist</span> <span class="o">-</span> <span class="n">W_m</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span>            <span class="c1"># a_bar from the lecture</span>
            <span class="n">Uy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">a_bar</span><span class="p">)</span>             <span class="c1"># U @ y_bar = L^{-1}</span>
            <span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">Uy</span><span class="p">)</span>             <span class="c1"># y_bar = U^{-1}L^{-1}</span>

            <span class="c1"># Reverse the order of y_bar with the matrix J</span>
            <span class="n">J</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="c1"># y_hist : concatenated y_m and y_bar</span>
            <span class="n">y_hist</span> <span class="o">=</span> <span class="n">J</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">y_bar</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span><span class="p">])</span>

            <span class="c1">#--------------------------------------------</span>
            <span class="c1"># Transform the optimal sequence back if β is given</span>
            <span class="c1">#--------------------------------------------</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">β</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">y_hist</span> <span class="o">=</span> <span class="n">y_hist</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">β</span><span class="o">**</span><span class="p">(</span><span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> \
                                    <span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">y_hist</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">y_bar</span>

        <span class="k">else</span><span class="p">:</span>           <span class="c1"># If the problem is stochastic and we look at it</span>

            <span class="n">Ea_hist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">a_hist</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">Ea_hist</span> <span class="o">=</span> <span class="n">J</span> <span class="o">@</span> <span class="n">Ea_hist</span>

            <span class="n">a_bar</span> <span class="o">=</span> <span class="n">Ea_hist</span> <span class="o">-</span> <span class="n">W_m</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span>           <span class="c1"># a_bar from the lecture</span>
            <span class="n">Uy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">a_bar</span><span class="p">)</span>             <span class="c1"># U @ y_bar = L^{-1}</span>
            <span class="n">y_bar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">Uy</span><span class="p">)</span>             <span class="c1"># y_bar = U^{-1}L^{-1}</span>

            <span class="c1"># Reverse the order of y_bar with the matrix J</span>
            <span class="n">J</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fliplr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
            <span class="c1"># y_hist : concatenated y_m and y_bar</span>
            <span class="n">y_hist</span> <span class="o">=</span> <span class="n">J</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">y_bar</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_m</span><span class="p">])</span>

            <span class="k">return</span> <span class="n">y_hist</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">y_bar</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s use this code to tackle two interesting examples.</p>
</section>
<section id="example-1">
<h3><span class="section-number">33.2.2. </span>Example 1<a class="headerlink" href="#example-1" title="Link to this heading">#</a></h3>
<p>Consider a stochastic process with moving average representation</p>
<div class="math notranslate nohighlight">
\[
x_t = (1 - 2 L) \varepsilon_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon_t\)</span> is a serially uncorrelated random process with mean zero and variance unity.</p>
<p>If we were to use the tools associated with infinite dimensional prediction and filtering to be described below,
we would use the Wiener-Kolmogorov formula <a class="reference internal" href="#equation-eq-36">(33.21)</a> to compute the linear least squares forecasts <span class="math notranslate nohighlight">\(\mathbb{E} [x_{t+j} \mid x_t, x_{t-1}, \ldots]\)</span>, for <span class="math notranslate nohighlight">\(j = 1,\, 2\)</span>.</p>
<p>But we can do everything we want by instead using our finite dimensional tools and
setting <span class="math notranslate nohighlight">\(d = r\)</span>, generating an instance of LQFilter, then invoking pertinent methods of LQFilter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">y_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">.0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">])</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">example</span> <span class="o">=</span> <span class="n">LQFilter</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y_m</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The Wold representation is computed by <code class="docutils literal notranslate"><span class="pre">example.coeffs_of_c()</span></code>.</p>
<p>Let’s check that it “flips roots” as required</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span><span class="o">.</span><span class="n">coeffs_of_c</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 2., -1.])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span><span class="o">.</span><span class="n">roots_of_characteristic</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([2.]), np.float64(-2.0), array([0.5]))
</pre></div>
</div>
</div>
</div>
<p>Now let’s form the covariance matrix of a time series vector of length <span class="math notranslate nohighlight">\(N\)</span>
and put it in <span class="math notranslate nohighlight">\(V\)</span>.</p>
<p>Then we’ll take a Cholesky decomposition of <span class="math notranslate nohighlight">\(V = L^{-1} L^{-1}\)</span> and use it to form the vector of
“moving average representations” <span class="math notranslate nohighlight">\(x = L^{-1} \varepsilon\)</span> and the vector of “autoregressive representations” <span class="math notranslate nohighlight">\(L x = \varepsilon\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="n">example</span><span class="o">.</span><span class="n">construct_V</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 5. -2.  0.  0.  0.]
 [-2.  5. -2.  0.  0.]
 [ 0. -2.  5. -2.  0.]
 [ 0.  0. -2.  5. -2.]
 [ 0.  0.  0. -2.  5.]]
</pre></div>
</div>
</div>
</div>
<p>Notice how the lower rows of the “moving average representations” are converging to the appropriate infinite history Wold representation
to be described below when we study infinite horizon-prediction and filtering</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Li</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Li</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 2.23606798  0.          0.          0.          0.        ]
 [-0.89442719  2.04939015  0.          0.          0.        ]
 [ 0.         -0.97590007  2.01186954  0.          0.        ]
 [ 0.          0.         -0.99410024  2.00293902  0.        ]
 [ 0.          0.          0.         -0.99853265  2.000733  ]]
</pre></div>
</div>
</div>
</div>
<p>Notice how the lower rows of the “autoregressive representations” are converging to the appropriate infinite-history
autoregressive representation to be described below when we study infinite horizon-prediction and filtering</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Li</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.4472136  0.         0.         0.         0.        ]
 [0.19518001 0.48795004 0.         0.         0.        ]
 [0.09467621 0.23669053 0.49705012 0.         0.        ]
 [0.04698977 0.11747443 0.2466963  0.49926632 0.        ]
 [0.02345182 0.05862954 0.12312203 0.24917554 0.49981682]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="example-2">
<h3><span class="section-number">33.2.3. </span>Example 2<a class="headerlink" href="#example-2" title="Link to this heading">#</a></h3>
<p>Consider a stochastic process <span class="math notranslate nohighlight">\(X_t\)</span> with moving average
representation</p>
<div class="math notranslate nohighlight">
\[
X_t = (1 - \sqrt 2 L^2) \varepsilon_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon_t\)</span> is a serially uncorrelated random process
with mean zero and variance unity.</p>
<p>Let’s find a Wold moving average representation for <span class="math notranslate nohighlight">\(x_t\)</span> that will prevail in the infinite-history context to be studied in
detail below.</p>
<p>To do this, we’ll  use the Wiener-Kolomogorov formula <a class="reference internal" href="#equation-eq-36">(33.21)</a> presented below to compute the linear least squares forecasts
<span class="math notranslate nohighlight">\(\mathbb{\hat E}\left[X_{t+j} \mid X_{t-1}, \ldots\right] \hbox { for } j = 1,\, 2,\, 3\)</span>.</p>
<p>We proceed in the same way as in example 1</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">y_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mf">.0</span><span class="p">,</span> <span class="mf">.0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)])</span>
<span class="n">h</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">example</span> <span class="o">=</span> <span class="n">LQFilter</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y_m</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
<span class="n">example</span><span class="o">.</span><span class="n">coeffs_of_c</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 1.41421356, -0.        , -1.        ])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">example</span><span class="o">.</span><span class="n">roots_of_characteristic</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([ 1.18920712, -1.18920712]),
 np.float64(-1.4142135623731122),
 array([ 0.84089642, -0.84089642]))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="n">example</span><span class="o">.</span><span class="n">construct_V</span><span class="p">(</span><span class="n">N</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 3.          0.         -1.41421356  0.          0.          0.
   0.          0.        ]
 [ 0.          3.          0.         -1.41421356  0.          0.
   0.          0.        ]
 [-1.41421356  0.          3.          0.         -1.41421356  0.
   0.          0.        ]
 [ 0.         -1.41421356  0.          3.          0.         -1.41421356
   0.          0.        ]
 [ 0.          0.         -1.41421356  0.          3.          0.
  -1.41421356  0.        ]
 [ 0.          0.          0.         -1.41421356  0.          3.
   0.         -1.41421356]
 [ 0.          0.          0.          0.         -1.41421356  0.
   3.          0.        ]
 [ 0.          0.          0.          0.          0.         -1.41421356
   0.          3.        ]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Li</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Li</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:,</span> <span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 0.          0.          0.         -0.9258201   0.          1.46385011
   0.          0.        ]
 [ 0.          0.          0.          0.         -0.96609178  0.
   1.43759058  0.        ]
 [ 0.          0.          0.          0.          0.         -0.96609178
   0.          1.43759058]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Li</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.57735027 0.         0.         0.         0.         0.
  0.         0.        ]
 [0.         0.57735027 0.         0.         0.         0.
  0.         0.        ]
 [0.3086067  0.         0.65465367 0.         0.         0.
  0.         0.        ]
 [0.         0.3086067  0.         0.65465367 0.         0.
  0.         0.        ]
 [0.19518001 0.         0.41403934 0.         0.68313005 0.
  0.         0.        ]
 [0.         0.19518001 0.         0.41403934 0.         0.68313005
  0.         0.        ]
 [0.13116517 0.         0.27824334 0.         0.45907809 0.
  0.69560834 0.        ]
 [0.         0.13116517 0.         0.27824334 0.         0.45907809
  0.         0.69560834]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="prediction">
<h3><span class="section-number">33.2.4. </span>Prediction<a class="headerlink" href="#prediction" title="Link to this heading">#</a></h3>
<p>It immediately follows from the “orthogonality principle” of least squares (see <span id="id8">[<a class="reference internal" href="zreferences.html#id112" title="Papoulis Athanasios and S Unnikrishna Pillai. Probability, random variables, and stochastic processes. Mc-Graw Hill, 1991.">Athanasios and Pillai, 1991</a>]</span> or <span id="id9">[<a class="reference internal" href="zreferences.html#id200" title="Thomas J Sargent. Macroeconomic Theory. Academic Press, New York, 2nd edition, 1987.">Sargent, 1987</a>]</span> [ch. X]) that</p>
<div class="math notranslate nohighlight" id="equation-eq-58">
<span class="eqno">(33.7)<a class="headerlink" href="#equation-eq-58" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    \mathbb{\hat E} &amp; [x_t \mid x_{t-m},\, x_{t-m+1}, \ldots x_1]
                    = \sum^{t-1}_{j=m} L^{-1}_{t,t-j}\, \varepsilon_{t-j} \\
               &amp; = [L_{t, 1}^{-1}\, L^{-1}_{t,2},\, \ldots, L^{-1}_{t,t-m}\ 0 \ 0 \ldots 0] L \, x
\end{aligned}\end{split}\]</div>
<p>This can be interpreted as a finite-dimensional version of the Wiener-Kolmogorov <span class="math notranslate nohighlight">\(m\)</span>-step ahead prediction formula.</p>
<p>We can use <a class="reference internal" href="#equation-eq-58">(33.7)</a>  to represent the linear least squares projection of
the vector <span class="math notranslate nohighlight">\(x\)</span> conditioned on the first <span class="math notranslate nohighlight">\(s\)</span> observations
<span class="math notranslate nohighlight">\([x_s, x_{s-1} \ldots, x_1]\)</span>.</p>
<p>We have</p>
<div class="math notranslate nohighlight" id="equation-eq-59">
<span class="eqno">(33.8)<a class="headerlink" href="#equation-eq-59" title="Link to this equation">#</a></span>\[\begin{split}\mathbb{\hat E}[x \mid x_s, x_{s-1}, \ldots, x_1]
= L^{-1}
\left[
    \begin{matrix}
        I_s &amp; 0 \\
        0 &amp; 0_{(t-s)}
    \end{matrix}
\right] L x\end{split}\]</div>
<p>This formula will be convenient in representing the solution of control problems under uncertainty.</p>
<p>Equation <a class="reference internal" href="#equation-eq-55">(33.4)</a>  can be recognized as a finite dimensional version of a moving average representation.</p>
<p>Equation  <a class="reference internal" href="#equation-eq-54">(33.2)</a> can be viewed as a finite dimension version of an autoregressive representation.</p>
<p>Notice that even
if the <span class="math notranslate nohighlight">\(x_t\)</span> process is covariance stationary, so that <span class="math notranslate nohighlight">\(V\)</span>
is such that <span class="math notranslate nohighlight">\(V_{ij}\)</span> depends only on <span class="math notranslate nohighlight">\(\vert i-j\vert\)</span>, the
coefficients in the moving average representation are time-dependent,
there being a different moving average for each <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>If
<span class="math notranslate nohighlight">\(x_t\)</span> is a covariance stationary process, the last row of
<span class="math notranslate nohighlight">\(L^{-1}\)</span> converges to the coefficients in the Wold moving average
representation for <span class="math notranslate nohighlight">\(\{ x_t\}\)</span> as <span class="math notranslate nohighlight">\(T \rightarrow \infty\)</span>.</p>
<p>Further, if <span class="math notranslate nohighlight">\(x_t\)</span> is covariance stationary, for fixed <span class="math notranslate nohighlight">\(k\)</span>
and <span class="math notranslate nohighlight">\(j &gt; 0, \, L^{-1}_{T,T-j}\)</span> converges to
<span class="math notranslate nohighlight">\(L^{-1}_{T-k, T-k-j}\)</span> as <span class="math notranslate nohighlight">\(T \rightarrow \infty\)</span>.</p>
<p>That is,
the “bottom” rows of <span class="math notranslate nohighlight">\(L^{-1}\)</span> converge to each other and to the
Wold moving average coefficients as <span class="math notranslate nohighlight">\(T \rightarrow \infty\)</span>.</p>
<p>This last observation gives one simple and widely-used practical way of
forming a finite <span class="math notranslate nohighlight">\(T\)</span> approximation to a Wold moving average
representation.</p>
<p>First, form the covariance matrix
<span class="math notranslate nohighlight">\(\mathbb{E}xx^\prime = V\)</span>, then obtain the Cholesky decomposition
<span class="math notranslate nohighlight">\(L^{-1} L^{-1^\prime}\)</span> of <span class="math notranslate nohighlight">\(V\)</span>, which can be accomplished
quickly on a computer.</p>
<p>The last row of <span class="math notranslate nohighlight">\(L^{-1}\)</span> gives the approximate Wold moving average coefficients.</p>
<p>This method can readily be generalized to multivariate systems.</p>
</section>
</section>
<section id="combined-finite-dimensional-control-and-prediction">
<span id="fdcp"></span><h2><span class="section-number">33.3. </span>Combined finite dimensional control and prediction<a class="headerlink" href="#combined-finite-dimensional-control-and-prediction" title="Link to this heading">#</a></h2>
<p>Consider the finite-dimensional control problem, maximize</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E} \, \sum^N_{t=0} \,
\left\{
     a_t y_t - {1 \over 2} h y^2_t - {1 \over 2} [d(L) y_t ]^2
\right\},\  \quad h &gt; 0
\]</div>
<p>where <span class="math notranslate nohighlight">\(d(L) = d_0 + d_1 L+ \ldots + d_m L^m\)</span>, <span class="math notranslate nohighlight">\(L\)</span> is the
lag operator, <span class="math notranslate nohighlight">\(\bar a = [ a_N, a_{N-1} \ldots, a_1, a_0]^\prime\)</span> a
random vector with mean zero and <span class="math notranslate nohighlight">\(\mathbb{E}\,\bar a \bar a^\prime = V\)</span>.</p>
<p>The variables <span class="math notranslate nohighlight">\(y_{-1}, \ldots, y_{-m}\)</span> are given.</p>
<p>Maximization is over choices of <span class="math notranslate nohighlight">\(y_0,
y_1 \ldots, y_N\)</span>, where <span class="math notranslate nohighlight">\(y_t\)</span> is required to be a linear function
of <span class="math notranslate nohighlight">\(\{y_{t-s-1}, t+m-1\geq 0;\ a_{t-s}, t\geq s\geq 0\}\)</span>.</p>
<p>We saw in the lecture <a class="reference internal" href="lu_tricks.html"><span class="doc">Classical Control with Linear Algebra</span></a>  that the solution of this problem under certainty could be represented in the feedback-feedforward form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
U \bar y
   = L^{-1}\bar a + K
   \left[
     \begin{matrix}
         y_{-1}\\
         \vdots\\
         y_{-m}
     \end{matrix}
   \right]
\end{split}\]</div>
<p>for some <span class="math notranslate nohighlight">\((N+1)\times m\)</span> matrix <span class="math notranslate nohighlight">\(K\)</span>.</p>
<p>Using a version of formula <a class="reference internal" href="#equation-eq-58">(33.7)</a>, we can express <span class="math notranslate nohighlight">\(\mathbb{\hat E}[\bar a \mid a_s,\, a_{s-1}, \ldots, a_0 ]\)</span> as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{\hat E}
[ \bar a \mid a_s,\, a_{s-1}, \ldots, a_0]
= \tilde U^{-1}
\left[
    \begin{matrix}
        0 &amp; 0 \\
        0 &amp; I_{(s+1)}
    \end{matrix}
\right]
\tilde U \bar a
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(I_{(s + 1)}\)</span> is the <span class="math notranslate nohighlight">\((s+1) \times (s+1)\)</span> identity
matrix, and <span class="math notranslate nohighlight">\(V = \tilde U^{-1} \tilde U^{-1^{\prime}}\)</span>, where
<span class="math notranslate nohighlight">\(\tilde U\)</span> is the <em>upper</em> triangular Cholesky factor of the
covariance matrix <span class="math notranslate nohighlight">\(V\)</span>.</p>
<p>(We have reversed the time axis in dating the <span class="math notranslate nohighlight">\(a\)</span>’s relative to earlier)</p>
<p>The time axis can be reversed in representation <a class="reference internal" href="#equation-eq-59">(33.8)</a> by replacing <span class="math notranslate nohighlight">\(L\)</span> with <span class="math notranslate nohighlight">\(L^T\)</span>.</p>
<p>The optimal decision rule to use at time <span class="math notranslate nohighlight">\(0 \leq t \leq N\)</span> is then
given by the <span class="math notranslate nohighlight">\((N-t +1)^{\rm th}\)</span> row of</p>
<div class="math notranslate nohighlight">
\[\begin{split}
U \bar y = L^{-1} \tilde U^{-1}
    \left[
        \begin{matrix}
            0 &amp; 0 \\
            0 &amp; I_{(t+1)}
        \end{matrix}
    \right]
    \tilde U \bar a + K
    \left[
    \begin{matrix}
        y_{-1}\\
        \vdots\\
        y_{-m}
    \end{matrix}
    \right]
\end{split}\]</div>
</section>
<section id="infinite-horizon-prediction-and-filtering-problems">
<h2><span class="section-number">33.4. </span>Infinite horizon prediction and filtering problems<a class="headerlink" href="#infinite-horizon-prediction-and-filtering-problems" title="Link to this heading">#</a></h2>
<p>It is instructive to compare the finite-horizon formulas based on linear algebra decompositions of finite-dimensional covariance matrices
with classic formulas for infinite horizon and infinite history prediction and control problems.</p>
<p>These classic infinite horizon formulas used the mathematics of <span class="math notranslate nohighlight">\(z\)</span>-transforms and lag operators.</p>
<p>We’ll meet interesting lag operator and <span class="math notranslate nohighlight">\(z\)</span>-transform  counterparts to our finite horizon matrix formulas.</p>
<p>We pose two related prediction and filtering problems.</p>
<p>We let <span class="math notranslate nohighlight">\(Y_t\)</span> be a univariate <span class="math notranslate nohighlight">\(m^{\rm th}\)</span> order moving average, covariance stationary stochastic process,</p>
<div class="math notranslate nohighlight" id="equation-eq-24">
<span class="eqno">(33.9)<a class="headerlink" href="#equation-eq-24" title="Link to this equation">#</a></span>\[Y_t = d(L) u_t\]</div>
<p>where <span class="math notranslate nohighlight">\(d(L) = \sum^m_{j=0} d_j L^j\)</span>, and <span class="math notranslate nohighlight">\(u_t\)</span> is a serially uncorrelated stationary random process satisfying</p>
<div class="math notranslate nohighlight" id="equation-eq-25">
<span class="eqno">(33.10)<a class="headerlink" href="#equation-eq-25" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    \mathbb{E} u_t &amp;= 0\\
    \mathbb{E} u_t u_s &amp;=
    \begin{cases}
        1 &amp; \text{ if } t = s \\
        0 &amp; \text{ otherwise}
    \end{cases}
\end{aligned}\end{split}\]</div>
<p>We impose no conditions on the zeros of <span class="math notranslate nohighlight">\(d(z)\)</span>.</p>
<p>A second covariance stationary process is <span class="math notranslate nohighlight">\(X_t\)</span> given by</p>
<div class="math notranslate nohighlight" id="equation-eq-26">
<span class="eqno">(33.11)<a class="headerlink" href="#equation-eq-26" title="Link to this equation">#</a></span>\[X_t = Y_t + \varepsilon_t\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon_t\)</span> is a serially uncorrelated stationary
random process with <span class="math notranslate nohighlight">\(\mathbb{E} \varepsilon_t = 0\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E} \varepsilon_t \varepsilon_s\)</span> = <span class="math notranslate nohighlight">\(0\)</span> for all distinct <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>We also assume that <span class="math notranslate nohighlight">\(\mathbb{E} \varepsilon_t u_s = 0\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>The <strong>linear least squares prediction problem</strong> is to find the <span class="math notranslate nohighlight">\(L_2\)</span>
random variable <span class="math notranslate nohighlight">\(\hat X_{t+j}\)</span> among linear combinations of
<span class="math notranslate nohighlight">\(\{ X_t,\  X_{t-1},
\ldots \}\)</span> that minimizes <span class="math notranslate nohighlight">\(\mathbb{E}(\hat X_{t+j} - X_{t+j})^2\)</span>.</p>
<p>That is, the problem is to find a <span class="math notranslate nohighlight">\(\gamma_j (L) = \sum^\infty_{k=0} \gamma_{jk}\, L^k\)</span> such that <span class="math notranslate nohighlight">\(\sum^\infty_{k=0} \vert \gamma_{jk} \vert^2 &lt; \infty\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E} [\gamma_j \, (L) X_t -X_{t+j}]^2\)</span> is minimized.</p>
<p>The <strong>linear least squares filtering problem</strong> is to find a <span class="math notranslate nohighlight">\(b\,(L) = \sum^\infty_{j=0} b_j\, L^j\)</span> such that <span class="math notranslate nohighlight">\(\sum^\infty_{j=0}\vert b_j \vert^2 &lt; \infty\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E} [b\, (L) X_t -Y_t ]^2\)</span> is minimized.</p>
<p>Interesting versions of these problems related to the permanent income theory were studied by <span id="id10">[<a class="reference internal" href="zreferences.html#id113" title="John F Muth. Optimal properties of exponentially weighted forecasts. Journal of the american statistical association, 55(290):299–306, 1960.">Muth, 1960</a>]</span>.</p>
<section id="problem-formulation">
<h3><span class="section-number">33.4.1. </span>Problem formulation<a class="headerlink" href="#problem-formulation" title="Link to this heading">#</a></h3>
<p>These problems are solved as follows.</p>
<p>The covariograms of <span class="math notranslate nohighlight">\(Y\)</span> and <span class="math notranslate nohighlight">\(X\)</span> and their cross covariogram are, respectively,</p>
<div class="math notranslate nohighlight" id="equation-eq-27">
<span class="eqno">(33.12)<a class="headerlink" href="#equation-eq-27" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    C_X (\tau) &amp;= \mathbb{E}X_t X_{t-\tau} \\
    C_Y (\tau) &amp;= \mathbb{E}Y_t Y_{t-\tau}  \qquad \tau = 0, \pm 1, \pm 2, \ldots \\
    C_{Y,X} (\tau) &amp;= \mathbb{E}Y_t X_{t-\tau}
\end{aligned}\end{split}\]</div>
<p>The covariance and cross-covariance generating functions are defined as</p>
<div class="math notranslate nohighlight" id="equation-eq-28">
<span class="eqno">(33.13)<a class="headerlink" href="#equation-eq-28" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    g_X(z) &amp;= \sum^\infty_{\tau = - \infty} C_X (\tau) z^\tau \\
    g_Y(z) &amp;= \sum^\infty_{\tau = - \infty} C_Y (\tau) z^\tau \\
    g_{YX} (z) &amp;= \sum^\infty_{\tau = - \infty} C_{YX} (\tau) z^\tau
\end{aligned}\end{split}\]</div>
<p>The generating functions can be computed by using the following facts.</p>
<p>Let <span class="math notranslate nohighlight">\(v_{1t}\)</span> and <span class="math notranslate nohighlight">\(v_{2t}\)</span> be two mutually and serially uncorrelated white noises with unit variances.</p>
<p>That is, <span class="math notranslate nohighlight">\(\mathbb{E}v^2_{1t} = \mathbb{E}v^2_{2t} = 1, \mathbb{E}v_{1t} = \mathbb{E}v_{2t} = 0, \mathbb{E}v_{1t} v_{2s} = 0\)</span> for all <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}v_{1t} v_{1t-j} = \mathbb{E}v_{2t} v_{2t-j} = 0\)</span> for all <span class="math notranslate nohighlight">\(j \not = 0\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(x_t\)</span> and <span class="math notranslate nohighlight">\(y_t\)</span> be two random processes given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    y_t &amp;= A(L) v_{1t} + B(L) v_{2t} \\
    x_t &amp;= C(L) v_{1t} + D(L) v_{2t}
\end{aligned}
\end{split}\]</div>
<p>Then, as shown for example in <span id="id11">[<a class="reference internal" href="zreferences.html#id200" title="Thomas J Sargent. Macroeconomic Theory. Academic Press, New York, 2nd edition, 1987.">Sargent, 1987</a>]</span> [ch. XI], it is true that</p>
<div class="math notranslate nohighlight" id="equation-eq-29">
<span class="eqno">(33.14)<a class="headerlink" href="#equation-eq-29" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    g_y(z) &amp;= A(z) A(z^{-1}) + B (z) B(z^{-1}) \\
    g_x (z) &amp;= C(z) C(z^{-1}) + D(z) D(z^{-1}) \\
    g_{yx} (z) &amp;= A(z) C(z^{-1}) + B(z) D(z^{-1})
\end{aligned}\end{split}\]</div>
<p>Applying these formulas to <a class="reference internal" href="#equation-eq-24">(33.9)</a> – <a class="reference internal" href="#equation-eq-27">(33.12)</a>, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-30">
<span class="eqno">(33.15)<a class="headerlink" href="#equation-eq-30" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    g_Y(z) &amp;= d(z)d(z^{-1}) \\
    g_X(z) &amp;= d(z)d(z^{-1}) + h\\
    g_{YX} (z) &amp;= d(z) d(z^{-1})
\end{aligned}\end{split}\]</div>
<p>The key step in obtaining solutions to our problems is to factor the covariance generating function  <span class="math notranslate nohighlight">\(g_X(z)\)</span> of <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>The solutions of our problems are given by formulas due to Wiener and Kolmogorov.</p>
<p>These formulas utilize the Wold moving average representation of the <span class="math notranslate nohighlight">\(X_t\)</span> process,</p>
<div class="math notranslate nohighlight" id="equation-eq-31">
<span class="eqno">(33.16)<a class="headerlink" href="#equation-eq-31" title="Link to this equation">#</a></span>\[X_t = c\,(L)\,\eta_t\]</div>
<p>where <span class="math notranslate nohighlight">\(c(L) = \sum^m_{j=0} c_j\, L^j\)</span>, with</p>
<div class="math notranslate nohighlight" id="equation-eq-32">
<span class="eqno">(33.17)<a class="headerlink" href="#equation-eq-32" title="Link to this equation">#</a></span>\[c_0 \eta_t
= X_t - \mathbb{\hat E} [X_t \vert X_{t-1}, X_{t-2}, \ldots]\]</div>
<p>Here <span class="math notranslate nohighlight">\(\mathbb{\hat E}\)</span> is the linear least squares projection operator.</p>
<p>Equation <a class="reference internal" href="#equation-eq-32">(33.17)</a>  is the condition that <span class="math notranslate nohighlight">\(c_0 \eta_t\)</span> can be the one-step-ahead error in predicting <span class="math notranslate nohighlight">\(X_t\)</span> from its own past values.</p>
<p>Condition <a class="reference internal" href="#equation-eq-32">(33.17)</a>  requires that <span class="math notranslate nohighlight">\(\eta_t\)</span> lie in the closed
linear space spanned by <span class="math notranslate nohighlight">\([X_t,\  X_{t-1}, \ldots]\)</span>.</p>
<p>This will be true if and only if the zeros of <span class="math notranslate nohighlight">\(c(z)\)</span> do not lie inside the unit circle.</p>
<p>It is an implication of <a class="reference internal" href="#equation-eq-32">(33.17)</a> that <span class="math notranslate nohighlight">\(\eta_t\)</span> is a serially
uncorrelated random process and that normalization can be imposed so
that <span class="math notranslate nohighlight">\(\mathbb{E}\eta_t^2 = 1\)</span>.</p>
<p>Consequently, an implication of <a class="reference internal" href="#equation-eq-31">(33.16)</a>  is
that the covariance generating function of <span class="math notranslate nohighlight">\(X_t\)</span> can be expressed
as</p>
<div class="math notranslate nohighlight" id="equation-eq-33">
<span class="eqno">(33.18)<a class="headerlink" href="#equation-eq-33" title="Link to this equation">#</a></span>\[g_X(z) = c\,(z)\,c\,(z^{-1})\]</div>
<p>It remains to discuss how <span class="math notranslate nohighlight">\(c(L)\)</span> is to be computed.</p>
<p>Combining <a class="reference internal" href="#equation-eq-29">(33.14)</a>  and <a class="reference internal" href="#equation-eq-33">(33.18)</a>  gives</p>
<div class="math notranslate nohighlight" id="equation-eq-34">
<span class="eqno">(33.19)<a class="headerlink" href="#equation-eq-34" title="Link to this equation">#</a></span>\[d(z) \,d(z^{-1}) + h = c \, (z) \,c\,(z^{-1})\]</div>
<p>Therefore, we have already shown constructively how to factor the covariance generating function <span class="math notranslate nohighlight">\(g_X(z) = d(z)\,d\,(z^{-1}) + h\)</span>.</p>
<p>We now introduce the <strong>annihilation operator</strong>:</p>
<div class="math notranslate nohighlight" id="equation-eq-35">
<span class="eqno">(33.20)<a class="headerlink" href="#equation-eq-35" title="Link to this equation">#</a></span>\[\left[
    \sum^\infty_{j= - \infty} f_j\, L^j
\right]_+
\equiv \sum^\infty_{j=0} f_j\,L^j\]</div>
<p>In words, <span class="math notranslate nohighlight">\([\phantom{00}]_+\)</span> means “ignore negative powers of <span class="math notranslate nohighlight">\(L\)</span>”.</p>
<p>We have defined the solution of the prediction problem as <span class="math notranslate nohighlight">\(\mathbb{\hat E} [X_{t+j} \vert X_t,\, X_{t-1}, \ldots] = \gamma_j\, (L) X_t\)</span>.</p>
<p>Assuming that the roots of <span class="math notranslate nohighlight">\(c(z) = 0\)</span> all lie outside the unit circle, the Wiener-Kolmogorov formula for <span class="math notranslate nohighlight">\(\gamma_j (L)\)</span> holds:</p>
<div class="math notranslate nohighlight" id="equation-eq-36">
<span class="eqno">(33.21)<a class="headerlink" href="#equation-eq-36" title="Link to this equation">#</a></span>\[\gamma_j\, (L) =
\left[
    {c (L) \over L^j}
\right]_+ c\,(L)^{-1}\]</div>
<p>We have defined the solution of the filtering problem as <span class="math notranslate nohighlight">\(\mathbb{\hat E}[Y_t \mid X_t, X_{t-1}, \ldots] = b (L)X_t\)</span>.</p>
<p>The Wiener-Kolomogorov formula for <span class="math notranslate nohighlight">\(b(L)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
b(L) = \left[{g_{YX} (L) \over c(L^{-1})}\right]_+ c(L)^{-1}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-37">
<span class="eqno">(33.22)<a class="headerlink" href="#equation-eq-37" title="Link to this equation">#</a></span>\[b(L) = \left[ {d(L)d(L^{-1}) \over c(L^{-1})} \right]_+ c(L)^{-1}\]</div>
<p>Formulas <a class="reference internal" href="#equation-eq-36">(33.21)</a> and <a class="reference internal" href="#equation-eq-37">(33.22)</a>  are discussed in detail in  <span id="id12">[<a class="reference internal" href="zreferences.html#id88" title="Peter Whittle. Prediction and Regulation by Linear Least Squares Methods. University of Minnesota Press, Minneapolis, Minnesota, 2nd edition, 1983.">Whittle, 1983</a>]</span> and <span id="id13">[<a class="reference internal" href="zreferences.html#id200" title="Thomas J Sargent. Macroeconomic Theory. Academic Press, New York, 2nd edition, 1987.">Sargent, 1987</a>]</span>.</p>
<p>The interested reader can there find several examples of the use of these formulas in economics
Some classic examples using these formulas are due to <span id="id14">[<a class="reference internal" href="zreferences.html#id113" title="John F Muth. Optimal properties of exponentially weighted forecasts. Journal of the american statistical association, 55(290):299–306, 1960.">Muth, 1960</a>]</span>.</p>
<p>As an example of the usefulness of formula <a class="reference internal" href="#equation-eq-37">(33.22)</a>, we let <span class="math notranslate nohighlight">\(X_t\)</span> be a stochastic process with Wold moving average representation</p>
<div class="math notranslate nohighlight">
\[
X_t = c(L) \eta_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{E}\eta^2_t = 1, \hbox { and } c_0 \eta_t = X_t - \mathbb{\hat E} [X_t \vert X_{t-1}, \ldots], c (L) = \sum^m_{j=0} c_j L\)</span>.</p>
<p>Suppose that at time <span class="math notranslate nohighlight">\(t\)</span>, we wish to predict a geometric sum of future <span class="math notranslate nohighlight">\(X\)</span>’s, namely</p>
<div class="math notranslate nohighlight">
\[
y_t \equiv \sum^\infty_{j=0} \delta^j X_{t+j} = {1 \over 1 - \delta L^{-1}}
X_t
\]</div>
<p>given knowledge of <span class="math notranslate nohighlight">\(X_t, X_{t-1}, \ldots\)</span>.</p>
<p>We shall use <a class="reference internal" href="#equation-eq-37">(33.22)</a>  to obtain the answer.</p>
<p>Using the standard formulas  <a class="reference internal" href="#equation-eq-29">(33.14)</a>, we have that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    g_{yx}(z) &amp;= (1-\delta z^{-1})c(z) c (z^{-1}) \\
    g_x (z) &amp;= c(z) c (z^{-1})
\end{aligned}
\end{split}\]</div>
<p>Then <a class="reference internal" href="#equation-eq-37">(33.22)</a>  becomes</p>
<div class="math notranslate nohighlight" id="equation-eq-38">
<span class="eqno">(33.23)<a class="headerlink" href="#equation-eq-38" title="Link to this equation">#</a></span>\[b(L)=\left[{c(L)\over 1-\delta L^{-1}}\right]_+ c(L)^{-1}\]</div>
<p>In order to evaluate the term in the annihilation operator, we use the following result from <span id="id15">[<a class="reference internal" href="zreferences.html#id110" title="Lars Peter Hansen and Thomas J Sargent. Formulating and estimating dynamic linear rational expectations models. Journal of Economic Dynamics and control, 2:7–46, 1980.">Hansen and Sargent, 1980</a>]</span>.</p>
<p><strong>Proposition</strong> Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(g(z) = \sum^\infty_{j=0} g_j \, z^j\)</span> where <span class="math notranslate nohighlight">\(\sum^\infty_{j=0} \vert g_j \vert^2 &lt; + \infty\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(h\,(z^{-1}) =\)</span> <span class="math notranslate nohighlight">\((1- \delta_1 z^{-1}) \ldots (1-\delta_n z^{-1})\)</span>, where <span class="math notranslate nohighlight">\(\vert \delta_j \vert &lt; 1\)</span>, for <span class="math notranslate nohighlight">\(j = 1, \ldots, n\)</span>.</p></li>
</ul>
<p>Then</p>
<div class="math notranslate nohighlight" id="equation-eq-39">
<span class="eqno">(33.24)<a class="headerlink" href="#equation-eq-39" title="Link to this equation">#</a></span>\[\left[{g(z)\over h(z^{-1})}\right]_+ = {g(z)\over h(z^{-1})} - \sum^n_{j=1}
\ {\delta_j g (\delta_j) \over \prod^n_{k=1 \atop k \not = j} (\delta_j -
\delta_k)} \ \left({1 \over z- \delta_j}\right)\]</div>
<p>and, alternatively,</p>
<div class="math notranslate nohighlight" id="equation-eq-40">
<span class="eqno">(33.25)<a class="headerlink" href="#equation-eq-40" title="Link to this equation">#</a></span>\[\left[
    {g(z)\over h(z^{-1})}
\right]_+
=\sum^n_{j=1} B_j
\left(
    {zg(z)-\delta_j g (\delta_j) \over z- \delta_j}
\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(B_j = 1 / \prod^n_{k=1\atop k+j} (1 - \delta_k / \delta_j)\)</span>.</p>
<p>Applying formula <a class="reference internal" href="#equation-eq-40">(33.25)</a>  of the proposition to evaluating  <a class="reference internal" href="#equation-eq-38">(33.23)</a>  with <span class="math notranslate nohighlight">\(g(z) = c(z)\)</span> and <span class="math notranslate nohighlight">\(h(z^{-1}) = 1 - \delta z^{-1}\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
b(L)=\left[{Lc(L)-\delta c(\delta)\over L-\delta}\right] c(L)^{-1}
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
b(L) =
\left[
    {1-\delta c (\delta) L^{-1} c (L)^{-1}\over 1-\delta L^{-1}}
\right]
\]</div>
<p>Thus, we have</p>
<div class="math notranslate nohighlight" id="equation-eq-41">
<span class="eqno">(33.26)<a class="headerlink" href="#equation-eq-41" title="Link to this equation">#</a></span>\[\mathbb{\hat E}
\left[
    \sum^\infty_{j=0} \delta^j X_{t+j}\vert X_t,\, x_{t-1},
    \ldots
\right] =
\left[
    {1-\delta c (\delta) L^{-1} c(L)^{-1} \over 1 - \delta L^{-1}}
\right]
\, X_t\]</div>
<p>This formula is useful in solving stochastic versions of problem 1 of lecture <a class="reference internal" href="lu_tricks.html"><span class="doc">Classical Control with Linear Algebra</span></a> in which the randomness emerges because <span class="math notranslate nohighlight">\(\{a_t\}\)</span> is a stochastic
process.</p>
<p>The problem is to maximize</p>
<div class="math notranslate nohighlight" id="equation-eq-42">
<span class="eqno">(33.27)<a class="headerlink" href="#equation-eq-42" title="Link to this equation">#</a></span>\[\mathbb{E}_0
\lim_{N \rightarrow \infty}\
\sum^N_{t-0} \beta^t
\left[
    a_t\, y_t - {1 \over 2}\ hy^2_t-{1 \over 2}\ [d(L)y_t]^2
\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{E}_t\)</span> is mathematical expectation conditioned on information
known at <span class="math notranslate nohighlight">\(t\)</span>, and where <span class="math notranslate nohighlight">\(\{ a_t\}\)</span> is a covariance
stationary stochastic process with Wold moving average representation</p>
<div class="math notranslate nohighlight">
\[
a_t = c(L)\, \eta_t
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
c(L) = \sum^{\tilde n}_{j=0} c_j L^j
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\eta_t =
a_t - \mathbb{\hat E} [a_t \vert a_{t-1}, \ldots]
\]</div>
<p>The problem is to maximize <a class="reference internal" href="#equation-eq-42">(33.27)</a>  with respect to a contingency plan
expressing <span class="math notranslate nohighlight">\(y_t\)</span> as a function of information known at <span class="math notranslate nohighlight">\(t\)</span>,
which is assumed to be
<span class="math notranslate nohighlight">\((y_{t-1},\  y_{t-2}, \ldots, a_t, \ a_{t-1}, \ldots)\)</span>.</p>
<p>The solution of this problem can be achieved in two steps.</p>
<p>First, ignoring the uncertainty, we can solve the problem assuming that <span class="math notranslate nohighlight">\(\{a_t\}\)</span> is a known sequence.</p>
<p>The solution is, from above,</p>
<div class="math notranslate nohighlight">
\[
c(L) y_t = c(\beta L^{-1})^{-1} a_t
\]</div>
<p>or</p>
<div class="math notranslate nohighlight" id="equation-eq-43">
<span class="eqno">(33.28)<a class="headerlink" href="#equation-eq-43" title="Link to this equation">#</a></span>\[(1-\lambda_1 L) \ldots (1 - \lambda_m L) y_t
= \sum^m_{j=1} A_j
\sum^\infty_{k=0} (\lambda_j \beta)^k\, a_{t+k}\]</div>
<p>Second, the solution of the problem under uncertainty is obtained by
replacing the terms on the right-hand side of the above expressions with
their linear least squares predictors.</p>
<p>Using <a class="reference internal" href="#equation-eq-41">(33.26)</a> and <a class="reference internal" href="#equation-eq-43">(33.28)</a>, we have
the following solution</p>
<div class="math notranslate nohighlight">
\[
(1-\lambda_1 L) \ldots (1-\lambda_m L) y_t =
\sum^m_{j=1} A_j
 \left[
     \frac{1-\beta \lambda_j \, c (\beta \lambda_j) L^{-1} c(L)^{-1} }
     { 1-\beta \lambda_j L^{-1} }
 \right] a_t
\]</div>
<p><strong>Blaschke factors</strong></p>
<p>The following is a useful piece of mathematics underlying “root flipping”.</p>
<p>Let <span class="math notranslate nohighlight">\(\pi (z) = \sum^m_{j=0} \pi_j z^j\)</span> and let <span class="math notranslate nohighlight">\(z_1, \ldots,
z_k\)</span> be the zeros of <span class="math notranslate nohighlight">\(\pi (z)\)</span> that are inside the unit circle, <span class="math notranslate nohighlight">\(k &lt; m\)</span>.</p>
<p>Then define</p>
<div class="math notranslate nohighlight">
\[
\theta (z) = \pi (z) \Biggl( {(z_1 z-1) \over (z-z_1)} \Biggr)
\Biggl( { (z_2 z-1) \over (z-z_2) } \Biggr ) \ldots \Biggl({(z_kz-1) \over
(z-z_k) }\Biggr)
\]</div>
<p>The term multiplying <span class="math notranslate nohighlight">\(\pi (z)\)</span> is termed a “Blaschke factor”.</p>
<p>Then it can be proved directly that</p>
<div class="math notranslate nohighlight">
\[
\theta (z^{-1}) \theta (z) = \pi (z^{-1}) \pi (z)
\]</div>
<p>and that the zeros of <span class="math notranslate nohighlight">\(\theta (z)\)</span> are not inside the unit circle.</p>
</section>
</section>
<section id="exercises">
<h2><span class="section-number">33.5. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="exercise admonition" id="cf_ex1">

<p class="admonition-title"><span class="caption-number">Exercise 33.1 </span></p>
<section id="exercise-content">
<p>Let <span class="math notranslate nohighlight">\(Y_t = (1 - 2 L ) u_t\)</span> where <span class="math notranslate nohighlight">\(u_t\)</span> is a mean zero
white noise with <span class="math notranslate nohighlight">\(\mathbb{E} u^2_t = 1\)</span>. Let</p>
<div class="math notranslate nohighlight">
\[
X_t = Y_t + \varepsilon_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon_t\)</span> is a serially uncorrelated white noise with
<span class="math notranslate nohighlight">\(\mathbb{E} \varepsilon^2_t = 9\)</span>, and <span class="math notranslate nohighlight">\(\mathbb{E} \varepsilon_t u_s = 0\)</span> for all
<span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>Find the Wold moving average representation for <span class="math notranslate nohighlight">\(X_t\)</span>.</p>
<p>Find a formula for the <span class="math notranslate nohighlight">\(A_{1j}\)</span>’s in</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E} \widehat X_{t+1} \mid X_t, X_{t-1}, \ldots = \sum^\infty_{j=0} A_{1j}
X_{t-j}
\]</div>
<p>Find a formula for the <span class="math notranslate nohighlight">\(A_{2j}\)</span>’s in</p>
<div class="math notranslate nohighlight">
\[
\mathbb{\hat E} X_{t+2} \mid X_t, X_{t-1}, \ldots = \sum^\infty_{j=0} A_{2j}
X_{t-j}
\]</div>
</section>
</div>
<div class="exercise admonition" id="cf_ex2">

<p class="admonition-title"><span class="caption-number">Exercise 33.2 </span></p>
<section id="exercise-content">
<p><strong>Multivariable Prediction:</strong> Let <span class="math notranslate nohighlight">\(Y_t\)</span> be an <span class="math notranslate nohighlight">\((n\times 1)\)</span>
vector stochastic process with moving average representation</p>
<div class="math notranslate nohighlight">
\[
Y_t = D(L) U_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(D(L) = \sum^m_{j=0} D_j L^J, D_j\)</span> an <span class="math notranslate nohighlight">\(n \times n\)</span>
matrix, <span class="math notranslate nohighlight">\(U_t\)</span> an <span class="math notranslate nohighlight">\((n \times 1)\)</span> vector white noise with
<span class="math notranslate nohighlight">\(\mathbb{E} U_t =0\)</span> for all <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E} U_t U_s' = 0\)</span> for all <span class="math notranslate nohighlight">\(s \neq t\)</span>,
and <span class="math notranslate nohighlight">\(\mathbb{E} U_t U_t' = I\)</span> for all <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\varepsilon_t\)</span> be an <span class="math notranslate nohighlight">\(n \times 1\)</span> vector white noise with mean <span class="math notranslate nohighlight">\(0\)</span> and contemporaneous covariance matrix <span class="math notranslate nohighlight">\(H\)</span>, where <span class="math notranslate nohighlight">\(H\)</span> is a positive definite matrix.</p>
<p>Let <span class="math notranslate nohighlight">\(X_t = Y_t +\varepsilon_t\)</span>.</p>
<p>Define the covariograms as <span class="math notranslate nohighlight">\(C_X
(\tau) = \mathbb{E} X_t X^\prime_{t-\tau}, C_Y (\tau) = \mathbb{E} Y_t Y^\prime_{t-\tau},
C_{YX} (\tau) = \mathbb{E} Y_t X^\prime_{t-\tau}\)</span>.</p>
<p>Then define the matrix
covariance generating function, as in <a class="reference internal" href="lu_tricks.html#equation-onetwenty">(32.21)</a>, only interpret all the
objects in <a class="reference internal" href="lu_tricks.html#equation-onetwenty">(32.21)</a> as matrices.</p>
<p>Show that the covariance generating functions are given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    g_y (z) &amp;= D (z) D (z^{-1})^\prime \\
    g_X (z) &amp;= D (z) D (z^{-1})^\prime + H \\
    g_{YX} (z) &amp;= D (z) D (z^{-1})^\prime
\end{aligned}
\end{split}\]</div>
<p>A factorization of <span class="math notranslate nohighlight">\(g_X (z)\)</span> can be found (see <span id="id16">[<a class="reference internal" href="zreferences.html#id86" title="Y. A. Rozanov. Stationary Random Processes. Holden-Day, San Francisco, 1967.">Rozanov, 1967</a>]</span> or <span id="id17">[<a class="reference internal" href="zreferences.html#id88" title="Peter Whittle. Prediction and Regulation by Linear Least Squares Methods. University of Minnesota Press, Minneapolis, Minnesota, 2nd edition, 1983.">Whittle, 1983</a>]</span>) of the form</p>
<div class="math notranslate nohighlight">
\[
D (z) D (z^{-1})^\prime + H = C (z) C (z^{-1})^\prime, \quad C (z) =
\sum^m_{j=0} C_j z^j
\]</div>
<p>where the zeros of <span class="math notranslate nohighlight">\(\vert C(z)\vert\)</span> do not lie inside the unit
circle.</p>
<p>A vector Wold moving average representation of <span class="math notranslate nohighlight">\(X_t\)</span> is then</p>
<div class="math notranslate nohighlight">
\[
X_t = C(L) \eta_t
\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta_t\)</span> is an <span class="math notranslate nohighlight">\((n\times 1)\)</span> vector white noise that
is “fundamental” for <span class="math notranslate nohighlight">\(X_t\)</span>.</p>
<p>That is, <span class="math notranslate nohighlight">\(X_t - \mathbb{\hat E}\left[X_t \mid X_{t-1}, X_{t-2}
\ldots\right] = C_0 \, \eta_t\)</span>.</p>
<p>The optimum predictor of <span class="math notranslate nohighlight">\(X_{t+j}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mathbb{\hat E} \left[X_{t+j} \mid X_t, X_{t-1}, \ldots\right]
 = \left[{C(L) \over L^j} \right]_+ \eta_t
\]</div>
<p>If <span class="math notranslate nohighlight">\(C(L)\)</span> is invertible, i.e., if the zeros of <span class="math notranslate nohighlight">\(\det\)</span>
<span class="math notranslate nohighlight">\(C(z)\)</span> lie strictly outside the unit circle, then this formula can
be written</p>
<div class="math notranslate nohighlight">
\[
\mathbb{\hat E} \left[X_{t+j} \mid X_t, X_{t-1}, \ldots\right]
    = \left[{C(L) \over L^J} \right]_+ C(L)^{-1}\, X_t
\]</div>
</section>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                    <p>A theme by <a href="https://quantecon.org">QuantEcon</a></p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="orth_proj.html">
   1. Orthogonal Projections and Their Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stationary_densities.html">
   2. Continuous State Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="muth_kalman.html">
   3. Reverse Engineering a la Muth
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_dp.html">
   4. Discrete State Dynamic Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cons_news.html">
   5. Information and Consumption Smoothing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="smoothing.html">
   6. Consumption Smoothing with Complete and Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="smoothing_tax.html">
   7. Tax Smoothing with Complete and Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_jump_lq.html">
   8. Markov Jump Linear Quadratic Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tax_smoothing_1.html">
   9. How to Pay for a War: Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tax_smoothing_2.html">
   10. How to Pay for a War: Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tax_smoothing_3.html">
   11. How to Pay for a War: Part 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lqramsey.html">
   12. Optimal Taxation in an LQ Economy
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="arellano.html">
   13. Default Risk and Income Fluctuations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="matsuyama.html">
   14. Globalization and Cycles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coase.html">
   15. Coase’s Theory of the Firm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="match_transport.html">
   16. Composite Sorting
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dynamic Linear Economies
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="hs_recursive_models.html">
   17. Recursive Models of Dynamic Linear Economies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="growth_in_dles.html">
   18. Growth in Dynamic Linear Economies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lucas_asset_pricing_dles.html">
   19. Lucas Asset Pricing Using DLE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="irfs_in_hall_model.html">
   20. IRFs in Hall Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="permanent_income_dles.html">
   21. Permanent Income Model using the DLE Class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rosen_schooling_model.html">
   22. Rosen Schooling Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cattle_cycles.html">
   23. Cattle Cycles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hs_invertibility_example.html">
   24. Shock Non Invertibility
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Risk, Model Uncertainty, and Robustness
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="five_preferences.html">
   25. Risk and Model Uncertainty
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="entropy.html">
   26. Etymology of Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="robustness.html">
   27. Robustness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rob_markov_perf.html">
   28. Robust Markov Perfect Equilibrium
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Time Series Models
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="arma.html">
   29. Covariance Stationary Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="estspec.html">
   30. Estimation of Spectra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="additive_functionals.html">
   31. Additive and Multiplicative Functionals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lu_tricks.html">
   32. Classical Control with Linear Algebra
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   33. Classical Prediction and Filtering With Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="knowing_forecasts_of_others.html">
   34. Knowing the Forecasts of Others
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lucas_model.html">
   35. Asset Pricing II: The Lucas Asset Pricing Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="asset_pricing_lph.html">
   36. Elementary Asset Pricing Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="black_litterman.html">
   37. Two Modifications of Mean-Variance Portfolio Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BCG_complete_mkts.html">
   38. Irrelevance of Capital Structures with Complete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BCG_incomplete_mkts.html">
   39. Equilibrium Capital Structures with Incomplete Markets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming Squared
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="un_insure.html">
   40. Optimal Unemployment Insurance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dyn_stack.html">
   41. Stackelberg Plans
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="calvo_machine_learn.html">
   42. Machine Learning a Ramsey Plan
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="calvo.html">
   43. Time Inconsistency of Ramsey Plans
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="calvo_abreu.html">
   44. Sustainable Plans for a Calvo Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="opt_tax_recur.html">
   45. Optimal Taxation with State-Contingent Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="amss.html">
   46. Optimal Taxation without State-Contingent Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="amss2.html">
   47. Fluctuating Interest Rates Deliver Fiscal Insurance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="amss3.html">
   48. Fiscal Risk and Government Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chang_ramsey.html">
   49. Competitive Equilibria of a Model of Chang
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chang_credible.html">
   50. Credible Government Policies in a Model of Chang
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   51. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   52. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="status.html">
   53. Execution Statistics
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                </ul>

                <ul class="qe-toolbar__links">
                    <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search..." aria-label="Search..." autocomplete="off" accesskey="k">
                            <i data-feather="search" id="search-icon"></i>
                        </form>
                    </li>
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="/_notebooks/classical_filtering.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li class="download-pdf" id="downloadButton"><i data-feather="file"></i></li>
                    <!--
                    # Enable if looking for link to specific document hosted on GitHub
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python-advanced.myst/blob/main/lectures/classical_filtering.md" download><i data-feather="github"></i></a></li>
                    -->
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python-advanced.myst" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="/_pdf/quantecon-python-advanced.pdf" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://colab.research.google.com/github/QuantEcon/lecture-python-advanced.notebooks/blob/main/classical_filtering.ipynb">Colab</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-python-advanced.notebooks" data-urlpath="tree/lecture-python-advanced.notebooks/classical_filtering.ipynb" data-branch=main>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://colab.research.google.com/github/QuantEcon/lecture-python-advanced.notebooks/blob/main/classical_filtering.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "classical_filtering";
                const repoURL = "https://github.com/QuantEcon/lecture-python-advanced.notebooks";
                const urlPath = "tree/lecture-python-advanced.notebooks/classical_filtering.ipynb";
                const branch = "main"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
  </body>
</html>