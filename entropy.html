

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>25. Etymology of Entropy &#8212; Advanced Quantitative Economics with Python</title>
    <script src="https://unpkg.com/@popperjs/core@2.9.2/dist/umd/popper.min.js"></script>
    <script src="https://unpkg.com/tippy.js@6.3.1/dist/tippy-bundle.umd.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/feather-icons/dist/feather.min.js"></script>
    
        <script>
            MathJax = {
            loader: {load: ['[tex]/boldsymbol', '[tex]/textmacros']},
            tex: {
                packages: {'[+]': ['boldsymbol', 'textmacros']},
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                processEscapes: true,
                macros: {
                    "argmax" : "arg\\,max",
                    "argmin" : "arg\\,min",
                    "col"    : "col",
                    "Span"   :  "span",
                    "epsilon": "\\varepsilon",
                    "EE": "\\mathbb{E}",
                    "PP": "\\mathbb{P}",
                    "RR": "\\mathbb{R}",
                    "NN": "\\mathbb{N}",
                    "ZZ": "\\mathbb{Z}",
                    "aA": "\\mathcal{A}",
                    "bB": "\\mathcal{B}",
                    "cC": "\\mathcal{C}",
                    "dD": "\\mathcal{D}",
                    "eE": "\\mathcal{E}",
                    "fF": "\\mathcal{F}",
                    "gG": "\\mathcal{G}",
                    "hH": "\\mathcal{H}",
                }
            },
            svg: {
                fontCache: 'global',
                scale: 0.92,
                displayAlign: "center",
            },
            };
        </script>
    
    
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/quantecon-book-theme.279dae03c5caae754d20501e3fa00bbf.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />


    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script src="_static/quantecon-book-theme.15b0c36fffe88f468997fa7b698991d3.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"argmax": "arg\\,max", "argmin": "arg\\,min", "col": "col", "Span": "span", "epsilon": "\\varepsilon", "EE": "\\mathbb{E}", "PP": "\\mathbb{P}", "RR": "\\mathbb{R}", "NN": "\\mathbb{N}", "ZZ": "\\mathbb{Z}", "aA": "\\mathcal{A}", "bB": "\\mathcal{B}", "cC": "\\mathcal{C}", "dD": "\\mathcal{D}", "eE": "\\mathcal{E}", "fF": "\\mathcal{F}", "gG": "\\mathcal{G}", "hH": "\\mathcal{H}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'entropy';</script>
    <link rel="canonical" href="https://python-advanced.quantecon.org/entropy.html" />
    <link rel="shortcut icon" href="_static/lectures-favicon.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="26. Robustness" href="robustness.html" />
    <link rel="prev" title="24. Risk and Model Uncertainty" href="five_preferences.html" />

<!-- Normal Meta Tags -->
<meta name="author" context="Thomas J. Sargent &amp; John Stachurski" />
<meta name="keywords" content="Python, QuantEcon, Quantitative Economics, Economics, Sloan, Alfred P. Sloan Foundation, Tom J. Sargent, John Stachurski" />
<meta name="description" content=This website presents a set of lectures on advanced quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski. />

<!-- Twitter tags -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@quantecon" />
<meta name="twitter:title" content="Etymology of Entropy"/>
<meta name="twitter:description" content="This website presents a set of lectures on advanced quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski.">
<meta name="twitter:creator" content="@quantecon">
<meta name="twitter:image" content="https://assets.quantecon.org/img/qe-twitter-logo.png">

<!-- Opengraph tags -->
<meta property="og:title" content="Etymology of Entropy" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://python-advanced.quantecon.org/entropy.html" />
<meta property="og:image" content="https://assets.quantecon.org/img/qe-og-logo.png" />
<meta property="og:description" content="This website presents a set of lectures on advanced quantitative economic modeling, designed and written by Thomas J. Sargent and John Stachurski." />
<meta property="og:site_name" content="Advanced Quantitative Economics with Python" />
<meta name="theme-color" content="#ffffff" />

  </head>
<body>


    <span id="top"></span>

    <div class="qe-wrapper">

        <div class="qe-main">

            <div class="qe-page" id=entropy>

                <div class="qe-page__toc">

                    <div class="inner">

                        
                        <div class="qe-page__toc-header">
                            On this page
                        </div>


                        <nav id="bd-toc-nav" class="qe-page__toc-nav">
                            <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#information-theory">25.1. Information Theory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-measure-of-unpredictability">25.2. A Measure of Unpredictability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">25.2.1. Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">25.2.2. Example</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-properties-of-entropy">25.3. Mathematical Properties of Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-entropy">25.4. Conditional Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#independence-as-maximum-conditional-entropy">25.5. Independence as Maximum Conditional Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#thermodynamics">25.6. Thermodynamics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-divergence">25.7. Statistical  Divergence</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-distributions">25.8. Continuous distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-entropy-and-gaussian-distributions">25.9. Relative entropy and Gaussian distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#von-neumann-entropy">25.10. Von Neumann Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backus-chernov-zin-entropy">25.11. Backus-Chernov-Zin Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wiener-kolmogorov-prediction-error-formula-as-entropy">25.12. Wiener-Kolmogorov Prediction Error Formula as Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multivariate-processes">25.13. Multivariate Processes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequency-domain-robust-control">25.14. Frequency Domain Robust Control</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relative-entropy-for-a-continuous-random-variable">25.15. Relative Entropy for a Continuous Random Variable</a></li>
</ul>
                            <p class="logo">
                                
                                    
                                    <a href=https://quantecon.org><img src="_static/qe-logo-large.png" class="logo" alt="logo"></a>
                                    
                                
                            </p>

                            <p class="powered">Powered by <a href="https://jupyterbook.org/">Jupyter Book</a></p>

                        </nav>

                        <div class="qe-page__toc-footer">
                            
                            
                            <p><a href="#top"><strong>Back to top</strong></a></p>
                        </div>

                    </div>

                </div>

                <div class="qe-page__header">

                    <div class="qe-page__header-copy">

                        <p class="qe-page__header-heading"><a href="intro.html">Advanced Quantitative Economics with Python</a></p>

                        <p class="qe-page__header-subheading">Etymology of Entropy</p>

                    </div>

                    <p class="qe-page__header-authors">Thomas J. Sargent & John Stachurski</p>

                </div> <!-- .page__header -->



                
                <main class="qe-page__content" role="main">
                    
                    <div>
                        
  <section class="tex2jax_ignore mathjax_ignore" id="etymology-of-entropy">
<h1><span class="section-number">25. </span>Etymology of Entropy<a class="headerlink" href="#etymology-of-entropy" title="Permalink to this heading">#</a></h1>
<p>This lecture describes and compares several notions of entropy.</p>
<p>Among the senses of entropy, we’ll encounter these</p>
<ul class="simple">
<li><p>A measure of <strong>uncertainty</strong> of a random variable advanced by Claude Shannon <span id="id1">[<a class="reference internal" href="zreferences.html#id3" title="Claude E. Shannon and Warren Weaver. The Mathematical Theory of Communication. University of Illinois Press, Urbana, 1949.">SW49</a>]</span></p></li>
<li><p>A key object governing thermodynamics</p></li>
<li><p>Kullback and Leibler’s measure of the statistical divergence between two probability distributions</p></li>
<li><p>A measure of the volatility of stochastic discount factors that appear in asset pricing theory</p></li>
<li><p>Measures of unpredictability that occur in classical Wiener-Kolmogorov linear prediction theory</p></li>
<li><p>A frequency domain criterion for constructing robust decision rules</p></li>
</ul>
<p>The concept of entropy plays an important role in robust control formulations described in this lecture
<a class="reference external" href="https://python-advanced.quantecon.org/five_preferences.html">Risk and Model Uncertainty</a> and in this lecture
<a class="reference external" href="https://python-advanced.quantecon.org/robustness.html">Robustness</a>.</p>
<section id="information-theory">
<h2><span class="section-number">25.1. </span>Information Theory<a class="headerlink" href="#information-theory" title="Permalink to this heading">#</a></h2>
<p>In information theory <span id="id2">[<a class="reference internal" href="zreferences.html#id3" title="Claude E. Shannon and Warren Weaver. The Mathematical Theory of Communication. University of Illinois Press, Urbana, 1949.">SW49</a>]</span>,  entropy is a measure of the unpredictability of a random variable.</p>
<p>To illustrate
things, let <span class="math notranslate nohighlight">\(X\)</span> be a discrete random variable taking values <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span>
with probabilities <span class="math notranslate nohighlight">\(p_i = \textrm{Prob}(X = x_i) \geq 0, \sum_i p_i =1\)</span>.</p>
<p>Claude Shannon’s <span id="id3">[<a class="reference internal" href="zreferences.html#id3" title="Claude E. Shannon and Warren Weaver. The Mathematical Theory of Communication. University of Illinois Press, Urbana, 1949.">SW49</a>]</span> definition  of entropy is</p>
<div class="math notranslate nohighlight" id="equation-eq-shannon1">
<span class="eqno">(25.1)<a class="headerlink" href="#equation-eq-shannon1" title="Permalink to this equation">#</a></span>\[ 
H(p) = \sum_i p_i \log_b (p_i^{-1}) = - \sum_i p_i \log_b (p_i) .
\]</div>
<p>where <span class="math notranslate nohighlight">\(\log_b\)</span> denotes the log function with base <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>Inspired by the limit</p>
<div class="math notranslate nohighlight">
\[
\lim_{p\downarrow 0} p \log p = \lim_{p \downarrow 0} \frac{\log p}{p^{-1}} = \lim_{p \downarrow 0}p = 0, 
\]</div>
<p>we set <span class="math notranslate nohighlight">\(p \log p = 0\)</span> in equation <a class="reference internal" href="#equation-eq-shannon1">(25.1)</a>.</p>
<p>Typical bases for the logarithm are <span class="math notranslate nohighlight">\(2\)</span>,  <span class="math notranslate nohighlight">\(e\)</span>, and <span class="math notranslate nohighlight">\(10\)</span>.</p>
<p>In the information theory literature, logarithms of base <span class="math notranslate nohighlight">\(2\)</span>, <span class="math notranslate nohighlight">\(e\)</span>, and <span class="math notranslate nohighlight">\(10\)</span> are associated with units of information
called bits, nats, and dits, respectively.</p>
<p>Shannon typically used base <span class="math notranslate nohighlight">\(2\)</span>.</p>
</section>
<section id="a-measure-of-unpredictability">
<h2><span class="section-number">25.2. </span>A Measure of Unpredictability<a class="headerlink" href="#a-measure-of-unpredictability" title="Permalink to this heading">#</a></h2>
<p>For a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> with probability density <span class="math notranslate nohighlight">\(p = \{p_i\}_{i=1}^n\)</span>,   the <strong>surprisal</strong>
for state <span class="math notranslate nohighlight">\(i\)</span> is  <span class="math notranslate nohighlight">\( s_i = \log\left(\frac{1}{p_i}\right) \)</span>.</p>
<p>The quantity <span class="math notranslate nohighlight">\( \log\left(\frac{1}{p_i}\right) \)</span> is called the <strong>surprisal</strong> because it is inversely related to the likelihood that state
<span class="math notranslate nohighlight">\(i\)</span> will occur.</p>
<p>Note that entropy <span class="math notranslate nohighlight">\(H(p)\)</span> equals the <strong>expected surprisal</strong></p>
<div class="math notranslate nohighlight">
\[ 
H(p) = \sum_i p_i s_i = \sum_i p_i \log\left(\frac{1}{p_i} \right) = -  \sum_i p_i \log\left(p_i \right).
\]</div>
<section id="example">
<h3><span class="section-number">25.2.1. </span>Example<a class="headerlink" href="#example" title="Permalink to this heading">#</a></h3>
<p>Take a possibly unfair coin, so <span class="math notranslate nohighlight">\(X = \{0,1\}\)</span> with <span class="math notranslate nohighlight">\(p = {\rm Prob}(X=1) = p \in [0,1]\)</span>.</p>
<p>Then</p>
<div class="math notranslate nohighlight">
\[ 
H(p) = -(1-p)\log (1-p) - p \log p. 
\]</div>
<p>Evidently,</p>
<div class="math notranslate nohighlight">
\[ 
H'(p) = \log(1-p) - \log p = 0 
\]</div>
<p>at <span class="math notranslate nohighlight">\(p=.5\)</span> and <span class="math notranslate nohighlight">\(H''(p) = -\frac{1}{1-p} -\frac{1}{p} &lt; 0\)</span> for <span class="math notranslate nohighlight">\(p\in (0,1)\)</span>.</p>
<p>So <span class="math notranslate nohighlight">\(p=.5\)</span> maximizes entropy, while entropy is minimized at <span class="math notranslate nohighlight">\(p=0\)</span> and <span class="math notranslate nohighlight">\(p=1\)</span>.</p>
<p>Thus, among all coins,  a fair coin is the most unpredictable.</p>
<p>See <a class="reference internal" href="#fig1"><span class="std std-numref">Fig. 25.1</span></a></p>
<figure class="align-default" id="fig1">
<a class="reference internal image-reference" href="_images/MyGraph5.png"><img alt="_images/MyGraph5.png" src="_images/MyGraph5.png" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 25.1 </span><span class="caption-text">Entropy as a function of <span class="math notranslate nohighlight">\(\hat \pi_1\)</span> when <span class="math notranslate nohighlight">\(\pi_1 = .5\)</span>.</span><a class="headerlink" href="#fig1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="id4">
<h3><span class="section-number">25.2.2. </span>Example<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>Take an <span class="math notranslate nohighlight">\(n\)</span>-sided possibly unfair die with  a probability distribution <span class="math notranslate nohighlight">\(\{p_i\}_{i=1}^n\)</span>.
The die is fair if <span class="math notranslate nohighlight">\(p_i = \frac{1}{n} \forall i\)</span>.</p>
<p>Among all dies, a fair die  maximizes entropy.</p>
<p>For a fair die,
entropy equals <span class="math notranslate nohighlight">\(H(p) = - n^{-1} \sum_i \log \left( \frac{1}{n} \right) = \log(n)\)</span>.</p>
<p>To specify the expected number of bits needed to isolate the outcome of one roll of a fair <span class="math notranslate nohighlight">\(n\)</span>-sided die requires <span class="math notranslate nohighlight">\(\log_2 (n)\)</span> bits of information.</p>
<p>For example,
if <span class="math notranslate nohighlight">\(n=2\)</span>, <span class="math notranslate nohighlight">\(\log_2(2) =1\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(n=3\)</span>, <span class="math notranslate nohighlight">\(\log_2(3) = 1.585\)</span>.</p>
</section>
</section>
<section id="mathematical-properties-of-entropy">
<h2><span class="section-number">25.3. </span>Mathematical Properties of Entropy<a class="headerlink" href="#mathematical-properties-of-entropy" title="Permalink to this heading">#</a></h2>
<p>For a discrete random variable with probability vector <span class="math notranslate nohighlight">\(p\)</span>, entropy <span class="math notranslate nohighlight">\(H(p)\)</span> is
a function that satisfies</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H\)</span> is <em>continuous</em>.</p></li>
<li><p><span class="math notranslate nohighlight">\(H\)</span> is <em>symmetric</em>: <span class="math notranslate nohighlight">\(H(p_1, p_2, \ldots, p_n) = H(p_{r_1}, \ldots, p_{r_n})\)</span> for any permutation <span class="math notranslate nohighlight">\(r_1, \ldots, r_n\)</span> of <span class="math notranslate nohighlight">\(1,\ldots, n\)</span>.</p></li>
<li><p>A uniform distribution maximizes <span class="math notranslate nohighlight">\(H(p)\)</span>: <span class="math notranslate nohighlight">\( H(p_1, \ldots, p_n) \leq H(\frac{1}{n}, \ldots, \frac{1}{n}) .\)</span></p></li>
<li><p>Maximum entropy increases with the number of states:
<span class="math notranslate nohighlight">\( H(\frac{1}{n}, \ldots, \frac{1}{n} ) \leq H(\frac{1}{n+1} , \ldots, \frac{1}{n+1})\)</span>.</p></li>
<li><p>Entropy is not affected by events zero probability.</p></li>
</ul>
</section>
<section id="conditional-entropy">
<h2><span class="section-number">25.4. </span>Conditional Entropy<a class="headerlink" href="#conditional-entropy" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\((X,Y)\)</span> be a bivariate discrete random vector with  outcomes <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span> and <span class="math notranslate nohighlight">\(y_1, \ldots, y_m\)</span>, respectively,
occurring with probability density <span class="math notranslate nohighlight">\(p(x_i, y_i)\)</span>.</p>
<p>Conditional entropy <span class="math notranslate nohighlight">\(H(X| Y)\)</span> is
defined as</p>
<div class="math notranslate nohighlight" id="equation-eq-shannon2">
<span class="eqno">(25.2)<a class="headerlink" href="#equation-eq-shannon2" title="Permalink to this equation">#</a></span>\[
H(X | Y) = \sum_{i,j} p(x_i,y_j) \log \frac{p(y_j)}{p(x_i,y_j)}.
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\frac{p(y_j)}{p(x_i,y_j)}\)</span>, the reciprocal of the conditional probability of <span class="math notranslate nohighlight">\(x_i\)</span> given <span class="math notranslate nohighlight">\(y_j\)</span>, can be defined as the <strong>conditional surprisal</strong>.</p>
</section>
<section id="independence-as-maximum-conditional-entropy">
<h2><span class="section-number">25.5. </span>Independence as Maximum Conditional Entropy<a class="headerlink" href="#independence-as-maximum-conditional-entropy" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(m=n\)</span> and <span class="math notranslate nohighlight">\([x_1, \ldots, x_n ] = [y_1, \ldots, y_n]\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\sum_j p(x_i,y_j) = \sum_j p(x_j, y_i) \)</span> for all <span class="math notranslate nohighlight">\(i\)</span>,
so that the marginal distributions of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are identical.</p>
<p>Thus, <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are identically distributed, but they
are not necessarily independent.</p>
<p>Consider the following problem:
choose a joint distribution  <span class="math notranslate nohighlight">\(p(x_i,y_j)\)</span> to maximize  conditional entropy
<a class="reference internal" href="#equation-eq-shannon2">(25.2)</a> subject to the restriction that  <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are identically distributed.</p>
<p>The conditional-entropy-maximizing  <span class="math notranslate nohighlight">\(p(x_i,y_j)\)</span> sets</p>
<div class="math notranslate nohighlight">
\[
\frac{p(x_i,y_j)}{p(y_j)} = \sum_j p(x_i, y_j) = p(x_i)  \forall i .
\]</div>
<p>Thus, among all joint distributions with identical marginal distributions,
the conditional entropy maximizing joint distribution makes <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> be
independent.</p>
</section>
<section id="thermodynamics">
<h2><span class="section-number">25.6. </span>Thermodynamics<a class="headerlink" href="#thermodynamics" title="Permalink to this heading">#</a></h2>
<p>Josiah Willard Gibbs (see <a class="reference external" href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs</a>) defined entropy as</p>
<div class="math notranslate nohighlight" id="equation-eq-gibbs">
<span class="eqno">(25.3)<a class="headerlink" href="#equation-eq-gibbs" title="Permalink to this equation">#</a></span>\[ 
 S = - k_B \sum_i p_i  \log p_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(p_i\)</span> is the probability of a micro state and <span class="math notranslate nohighlight">\(k_B\)</span> is Boltzmann’s constant.</p>
<ul class="simple">
<li><p>The Boltzmann constant <span class="math notranslate nohighlight">\(k_b\)</span> relates energy at the micro  particle level with the temperature observed at the macro level. It equals what is called a gas constant  divided by an Avogadro constant.</p></li>
</ul>
<p>The second law of thermodynamics states that the entropy of a closed physical system increases until <span class="math notranslate nohighlight">\(S\)</span> defined in <a class="reference internal" href="#equation-eq-gibbs">(25.3)</a> attains a maximum.</p>
</section>
<section id="statistical-divergence">
<h2><span class="section-number">25.7. </span>Statistical  Divergence<a class="headerlink" href="#statistical-divergence" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a discrete state space <span class="math notranslate nohighlight">\(x_1, \ldots, x_n\)</span> and let <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> be  two discrete probability
distributions on <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>Assume that <span class="math notranslate nohighlight">\(\frac{p_i}{q_t} \in (0,\infty)\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> for which <span class="math notranslate nohighlight">\(p_i &gt;0\)</span>.</p>
<p>Then the Kullback-Leibler statistical divergence, also called <strong>relative entropy</strong>,
is defined as</p>
<div class="math notranslate nohighlight" id="equation-eq-shanno3">
<span class="eqno">(25.4)<a class="headerlink" href="#equation-eq-shanno3" title="Permalink to this equation">#</a></span>\[
D(p|q) = \sum_i p_i \log \left(\frac{p_i}{q_i}\right) = \sum_i q_i \left( \frac{p_i}{q_i}\right) \log\left( \frac{p_i}{q_i}\right) .
\]</div>
<p>Evidently,</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
D(p|q) &amp; = - \sum_i p_i \log q_i + \sum_i p_i \log p_i \cr
  &amp; =  H(p,q) - H(p)   ,
  \end{aligned}
\]</div>
<p>where <span class="math notranslate nohighlight">\(H(p,q) = \sum_i p_i \log   q_i\)</span> is the cross-entropy.</p>
<p>It is easy to verify, as we have done above, that <span class="math notranslate nohighlight">\(
D(p|q) \geq 0\)</span> and that <span class="math notranslate nohighlight">\(D(p|q) = 0\)</span> implies that <span class="math notranslate nohighlight">\(p_i = q_i\)</span> when <span class="math notranslate nohighlight">\(q_i &gt;0\)</span>.</p>
</section>
<section id="continuous-distributions">
<h2><span class="section-number">25.8. </span>Continuous distributions<a class="headerlink" href="#continuous-distributions" title="Permalink to this heading">#</a></h2>
<p>For a continuous random variable, Kullback-Leibler divergence between two densities <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[ 
D(p|q) = \int p(x) \log \left(\frac{p(x)}{q(x)} \right) d \, x .
\]</div>
</section>
<section id="relative-entropy-and-gaussian-distributions">
<h2><span class="section-number">25.9. </span>Relative entropy and Gaussian distributions<a class="headerlink" href="#relative-entropy-and-gaussian-distributions" title="Permalink to this heading">#</a></h2>
<p>We want to compute relative entropy for two continuous densities <span class="math notranslate nohighlight">\(\phi\)</span> and <span class="math notranslate nohighlight">\(\hat \phi\)</span> when
<span class="math notranslate nohighlight">\(\phi\)</span> is <span class="math notranslate nohighlight">\({\cal N}(0,I)\)</span> and  <span class="math notranslate nohighlight">\({\hat \phi}\)</span> is <span class="math notranslate nohighlight">\({\cal N}(w, \Sigma)\)</span>, where the covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span> is nonsingular.</p>
<p>We seek a formula for</p>
<div class="math notranslate nohighlight">
\[ 
\textrm{ent} = \int (\log {\hat \phi(\varepsilon)} - \log \phi(\varepsilon) ){\hat \phi(\varepsilon)} d \varepsilon. 
\]</div>
<p><strong>Claim</strong></p>
<div class="math notranslate nohighlight" id="equation-eq-relentropy101">
<span class="eqno">(25.5)<a class="headerlink" href="#equation-eq-relentropy101" title="Permalink to this equation">#</a></span>\[
\textrm{ent} = %\int (\log {\hat \phi} - \log \phi ){\hat \phi} d \varepsilon=
-{1 \over 2} \log
\det \Sigma + {1 \over 2}w'w + {1 \over 2}\mathrm{trace} (\Sigma - I)
.
\]</div>
<p><strong>Proof</strong></p>
<p>The log likelihood ratio is</p>
<div class="math notranslate nohighlight" id="equation-footnote2">
<span class="eqno">(25.6)<a class="headerlink" href="#equation-footnote2" title="Permalink to this equation">#</a></span>\[ 
\log {\hat \phi}(\varepsilon) - \log \phi(\varepsilon) =
{1 \over 2} \left[ - (\varepsilon - w)' \Sigma^{-1} (\varepsilon - w)
    +  \varepsilon' \varepsilon - \log \det
    \Sigma\right] .
\]</div>
<p>Observe that</p>
<div class="math notranslate nohighlight">
\[
- \int {1 \over 2} (\varepsilon - w)' \Sigma^{-1} (\varepsilon -
w) {\hat \phi}(\varepsilon) d\varepsilon = - {1 \over 2}\mathrm{trace}(I).
\]</div>
<p>Applying the identity <span class="math notranslate nohighlight">\(\varepsilon = w + (\varepsilon - w)\)</span> gives</p>
<div class="math notranslate nohighlight">
\[
{1\over 2}\varepsilon' \varepsilon = {1 \over 2}w' w + {1 \over 2}
(\varepsilon - w)' (\varepsilon - w) +  w' (\varepsilon - w).
\]</div>
<p>Taking mathematical expectations</p>
<div class="math notranslate nohighlight">
\[
{1 \over 2} \int \varepsilon' \varepsilon {\hat \phi}(\varepsilon) d
\varepsilon = {1\over 2} w'w + {1 \over 2} \mathrm{trace}(\Sigma).
\]</div>
<p>Combining terms gives</p>
<div class="math notranslate nohighlight" id="equation-eq-relentropy">
<span class="eqno">(25.7)<a class="headerlink" href="#equation-eq-relentropy" title="Permalink to this equation">#</a></span>\[
\textrm{ent} = \int (\log {\hat \phi} - \log \phi ){\hat \phi} d \varepsilon= -{1 \over 2} \log
\det \Sigma + {1 \over 2}w'w + {1 \over 2}\mathrm{trace} (\Sigma - I)
.
\]</div>
<p>which agrees with equation <a class="reference internal" href="#equation-eq-relentropy101">(25.5)</a>.
Notice the separate  appearances of the mean distortion <span class="math notranslate nohighlight">\(w\)</span> and the covariance distortion
<span class="math notranslate nohighlight">\(\Sigma - I\)</span> in equation <a class="reference internal" href="#equation-eq-relentropy">(25.7)</a>.</p>
<p><strong>Extension</strong></p>
<p>Let  <span class="math notranslate nohighlight">\(N_0 = {\mathcal N}(\mu_0,\Sigma_0)\)</span> and <span class="math notranslate nohighlight">\(N_1={\mathcal N}(\mu_1, \Sigma_1)\)</span> be two multivariate Gaussian
distributions.</p>
<p>Then</p>
<div class="math notranslate nohighlight" id="equation-eq-shannon5">
<span class="eqno">(25.8)<a class="headerlink" href="#equation-eq-shannon5" title="Permalink to this equation">#</a></span>\[
D(N_0|N_1) = \frac{1}{2} \left(\mathrm {trace} (\Sigma_1^{-1} \Sigma_0)
+ (\mu_1 -\mu_0)' \Sigma_1^{-1} (\mu_1 - \mu_0) - \log\left( \frac{ \mathrm {det }\Sigma_0 }{\mathrm {det}\Sigma_1}\right)
   - k \right).
\]</div>
</section>
<section id="von-neumann-entropy">
<h2><span class="section-number">25.10. </span>Von Neumann Entropy<a class="headerlink" href="#von-neumann-entropy" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> be two positive-definite symmetric matrices.</p>
<p>A measure of the divergence between two <span class="math notranslate nohighlight">\(P\)</span> and <span class="math notranslate nohighlight">\(Q\)</span> is</p>
<div class="math notranslate nohighlight">
\[
D(P|Q)= \textrm{trace} ( P \ln P - P \ln Q - P + Q) 
\]</div>
<p>where the log of a matrix is defined here  (<a class="reference external" href="https://en.wikipedia.org/wiki/Logarithm_of_a_matrix">https://en.wikipedia.org/wiki/Logarithm_of_a_matrix</a>).</p>
<p>A density matrix <span class="math notranslate nohighlight">\(P\)</span> from quantum mechanics is a positive definite matrix with trace <span class="math notranslate nohighlight">\(1\)</span>.</p>
<p>The von Neumann entropy of a density matrix <span class="math notranslate nohighlight">\(P\)</span> is</p>
<div class="math notranslate nohighlight">
\[
S = - \textrm{trace} (P \ln P)
\]</div>
</section>
<section id="backus-chernov-zin-entropy">
<h2><span class="section-number">25.11. </span>Backus-Chernov-Zin Entropy<a class="headerlink" href="#backus-chernov-zin-entropy" title="Permalink to this heading">#</a></h2>
<p>After flipping signs, <span id="id5">[<a class="reference internal" href="zreferences.html#id4" title="David Backus, Mikhail Chernov, and Stanley Zin. Sources of Entropy in Representative Agent Models. Journal of Finance, 69(1):51-99, February 2014. URL: https://ideas.repec.org/a/bla/jfinan/v69y2014i1p51-99.html, doi:.">BCZ14</a>]</span>  use Kullback-Leibler relative entropy as a measure of volatility of stochastic discount factors that they
assert is useful for characterizing features of both the data and various theoretical models of stochastic discount factors.</p>
<p>Where <span class="math notranslate nohighlight">\(p_{t+1}\)</span> is the physical or true measure, <span class="math notranslate nohighlight">\(p_{t+1}^*\)</span> is the risk-neutral measure, and <span class="math notranslate nohighlight">\(E_t\)</span> denotes conditional
expectation under the <span class="math notranslate nohighlight">\(p_{t+1}\)</span> measure, <span id="id6">[<a class="reference internal" href="zreferences.html#id4" title="David Backus, Mikhail Chernov, and Stanley Zin. Sources of Entropy in Representative Agent Models. Journal of Finance, 69(1):51-99, February 2014. URL: https://ideas.repec.org/a/bla/jfinan/v69y2014i1p51-99.html, doi:.">BCZ14</a>]</span> define entropy as</p>
<div class="math notranslate nohighlight" id="equation-eq-bcz1">
<span class="eqno">(25.9)<a class="headerlink" href="#equation-eq-bcz1" title="Permalink to this equation">#</a></span>\[
L_t (p_{t+1}^*/p_{t+1}) = - E_t \log( p_{t+1}^*/p_{t+1}).
\]</div>
<p>Evidently, by virtue of the minus sign in equation <a class="reference internal" href="#equation-eq-bcz1">(25.9)</a>,</p>
<div class="math notranslate nohighlight" id="equation-eq-bcz2">
<span class="eqno">(25.10)<a class="headerlink" href="#equation-eq-bcz2" title="Permalink to this equation">#</a></span>\[
L_t (p_{t+1}^*/p_{t+1})  = D_{KL,t}( p_{t+1}^*|p_{t+1}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(D_{KL,t}\)</span> denotes conditional relative entropy.</p>
<p>Let <span class="math notranslate nohighlight">\(m_{t+1}\)</span> be a stochastic discount factor, <span class="math notranslate nohighlight">\(r_{t+1}\)</span> a gross one-period return on a risky
security, and <span class="math notranslate nohighlight">\((r_{t+1}^1)^{-1}\equiv q_t^1 = E_t m_{t+1}\)</span> be the reciprocal of a risk-free one-period gross rate of return.
Then</p>
<div class="math notranslate nohighlight">
\[
E_t (m_{t+1} r_{t+1}) = 1 
\]</div>
<p><span id="id7">[<a class="reference internal" href="zreferences.html#id4" title="David Backus, Mikhail Chernov, and Stanley Zin. Sources of Entropy in Representative Agent Models. Journal of Finance, 69(1):51-99, February 2014. URL: https://ideas.repec.org/a/bla/jfinan/v69y2014i1p51-99.html, doi:.">BCZ14</a>]</span> note that a stochastic discount factor satisfies</p>
<div class="math notranslate nohighlight">
\[ 
m_{t+1} = q_t^1 p_{t+1}^*/p_{t+1} .
\]</div>
<p>They derive the following <strong>entropy bound</strong></p>
<div class="math notranslate nohighlight">
\[
E L_t (m_{t+1}) \geq E (\log r_{t+1} - \log r_{t+1}^1 )
\]</div>
<p>which they propose as a complement to a Hansen-Jagannathan <span id="id8">[<a class="reference internal" href="zreferences.html#id20" title="Lars Peter Hansen and Ravi Jagannathan. Implications of Security Market Data for Models of Dynamic Economies. Journal of Political Economy, 99(2):225-262, April 1991. URL: https://ideas.repec.org/a/ucp/jpolec/v99y1991i2p225-62.html, doi:10.1086/261749.">HJ91</a>]</span> bound.</p>
</section>
<section id="wiener-kolmogorov-prediction-error-formula-as-entropy">
<h2><span class="section-number">25.12. </span>Wiener-Kolmogorov Prediction Error Formula as Entropy<a class="headerlink" href="#wiener-kolmogorov-prediction-error-formula-as-entropy" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\{x_t\}_{t=-\infty}^\infty\)</span> be a covariance stationary stochastic process with
mean zero and spectral density <span class="math notranslate nohighlight">\(S_x(\omega)\)</span>.</p>
<p>The variance of <span class="math notranslate nohighlight">\(x\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\sigma_x^2 =\left( \frac{1}{2\pi}\right) \int_{-\pi}^\pi  S_x (\omega) d \omega .
\]</div>
<p>As described  in chapter XIV of  <span id="id9">[<a class="reference internal" href="zreferences.html#id197" title="Thomas J Sargent. Macroeconomic Theory. Academic Press, New York, 2nd edition, 1987.">Sar87</a>]</span>, the Wiener-Kolmogorov formula for the one-period ahead prediction error is</p>
<div class="math notranslate nohighlight" id="equation-eq-shannon6">
<span class="eqno">(25.11)<a class="headerlink" href="#equation-eq-shannon6" title="Permalink to this equation">#</a></span>\[
\sigma_\epsilon^2 = \exp\left[\left( \frac{1}{2\pi}\right) \int_{-\pi}^\pi \log S_x (\omega) d \omega \right].
\]</div>
<p>Occasionally the logarithm of  the one-step-ahead prediction error <span class="math notranslate nohighlight">\(\sigma_\epsilon^2\)</span>
is called entropy because it measures unpredictability.</p>
<p>Consider the following problem  reminiscent of one  described earlier.</p>
<p><strong>Problem:</strong></p>
<p>Among all covariance stationary univariate processes with unconditional variance <span class="math notranslate nohighlight">\(\sigma_x^2\)</span>, find a process with maximal
one-step-ahead prediction error.</p>
<p>The maximizer  is  a process with spectral density</p>
<div class="math notranslate nohighlight">
\[
S_x(\omega) = 2 \pi \sigma_x^2.
\]</div>
<p>Thus,  among
all univariate covariance stationary processes with variance <span class="math notranslate nohighlight">\(\sigma_x^2\)</span>, a process with a flat spectral density is the most uncertain, in the sense of one-step-ahead prediction error variance.</p>
<p>This no-patterns-across-time outcome for a temporally dependent process resembles the no-pattern-across-states outcome for the static entropy maximizing coin or die  in the classic information theoretic
analysis described above.</p>
</section>
<section id="multivariate-processes">
<h2><span class="section-number">25.13. </span>Multivariate Processes<a class="headerlink" href="#multivariate-processes" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(y_t\)</span> be an <span class="math notranslate nohighlight">\(n \times 1\)</span> covariance stationary stochastic process with mean <span class="math notranslate nohighlight">\(0\)</span> with
matrix covariogram <span class="math notranslate nohighlight">\(C_y(j) = E y_t y_{t-j}' \)</span> and spectral density matrix</p>
<div class="math notranslate nohighlight">
\[
S_y(\omega) = \sum_{j=-\infty}^\infty e^{- i \omega j} C_y(j), \quad \omega \in [-\pi, \pi].  
\]</div>
<p>Let</p>
<div class="math notranslate nohighlight">
\[
y_t = D(L) \epsilon_t  \equiv \sum_{j=0}^\infty D_j \epsilon_t 
\]</div>
<p>be a Wold representation for <span class="math notranslate nohighlight">\(y\)</span>, where <span class="math notranslate nohighlight">\(D(0)\epsilon_t\)</span> is a
vector of one-step-ahead errors in predicting <span class="math notranslate nohighlight">\(y_t\)</span> conditional on the infinite history <span class="math notranslate nohighlight">\(y^{t-1} = [y_{t-1}, y_{t-2}, \ldots ]\)</span> and
<span class="math notranslate nohighlight">\(\epsilon_t\)</span> is an <span class="math notranslate nohighlight">\(n\times 1\)</span> vector of serially uncorrelated random disturbances with mean zero and identity contemporaneous
covariance matrix <span class="math notranslate nohighlight">\(E \epsilon_t \epsilon_t' = I\)</span>.</p>
<p>Linear-least-squares predictors have one-step-ahead prediction error <span class="math notranslate nohighlight">\(D(0)  D(0)'\)</span> that satisfies</p>
<div class="math notranslate nohighlight" id="equation-eq-shannon22">
<span class="eqno">(25.12)<a class="headerlink" href="#equation-eq-shannon22" title="Permalink to this equation">#</a></span>\[
\log \det [D(0) D(0)'] = \left(\frac{1}{2 \pi} \right) \int_{-\pi}^\pi \log \det [S_y(\omega)] d \omega.
\]</div>
<p>Being a  measure of the unpredictability of an <span class="math notranslate nohighlight">\(n \times 1\)</span> vector covariance stationary  stochastic process,
the left side of  <a class="reference internal" href="#equation-eq-shannon22">(25.12)</a>  is sometimes called entropy.</p>
</section>
<section id="frequency-domain-robust-control">
<h2><span class="section-number">25.14. </span>Frequency Domain Robust Control<a class="headerlink" href="#frequency-domain-robust-control" title="Permalink to this heading">#</a></h2>
<p>Chapter 8 of <span id="id10">[<a class="reference internal" href="zreferences.html#id105" title="Lars Peter Hansen and Thomas J Sargent. Robustness. Princeton University Press, 2008.">HS08b</a>]</span>  adapts work in the control theory literature to define a
<strong>frequency domain entropy</strong> criterion for  robust control as</p>
<div class="math notranslate nohighlight" id="equation-eq-shannon21">
<span class="eqno">(25.13)<a class="headerlink" href="#equation-eq-shannon21" title="Permalink to this equation">#</a></span>\[
\int_\Gamma \log \det [ \theta I - G_F(\zeta)' G_F(\zeta) ] d \lambda(\zeta) ,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta \in (\underline \theta, +\infty)\)</span> is a positive robustness parameter and <span class="math notranslate nohighlight">\(G_F(\zeta)\)</span> is a <span class="math notranslate nohighlight">\(\zeta\)</span>-transform of the
objective function.</p>
<p>Hansen and Sargent <span id="id11">[<a class="reference internal" href="zreferences.html#id105" title="Lars Peter Hansen and Thomas J Sargent. Robustness. Princeton University Press, 2008.">HS08b</a>]</span> show that criterion <a class="reference internal" href="#equation-eq-shannon21">(25.13)</a>  can be represented as</p>
<div class="math notranslate nohighlight" id="equation-eq-shannon220">
<span class="eqno">(25.14)<a class="headerlink" href="#equation-eq-shannon220" title="Permalink to this equation">#</a></span>\[ 
\log \det [ D(0)' D(0)] = \int_\Gamma \log \det [ \theta I - G_F(\zeta)' G_F(\zeta) ] d \lambda(\zeta) ,
\]</div>
<p>for an appropriate covariance stationary stochastic process derived from <span class="math notranslate nohighlight">\(\theta, G_F(\zeta)\)</span>.</p>
<p>This explains the
moniker <strong>maximum entropy</strong> robust control for decision rules <span class="math notranslate nohighlight">\(F\)</span> designed to maximize  criterion <a class="reference internal" href="#equation-eq-shannon21">(25.13)</a>.</p>
</section>
<section id="relative-entropy-for-a-continuous-random-variable">
<h2><span class="section-number">25.15. </span>Relative Entropy for a Continuous Random Variable<a class="headerlink" href="#relative-entropy-for-a-continuous-random-variable" title="Permalink to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(x\)</span> be  a continuous random variable with density <span class="math notranslate nohighlight">\(\phi(x)\)</span>, and let <span class="math notranslate nohighlight">\(g(x) \)</span> be a nonnegative random variable satisfying <span class="math notranslate nohighlight">\(\int g(x) \phi(x) dx =1\)</span>.</p>
<p>The relative entropy of the distorted density <span class="math notranslate nohighlight">\(\hat \phi(x) = g(x) \phi(x)\)</span>  is defined
as</p>
<div class="math notranslate nohighlight">
\[
\textrm{ent}(g) = \int g(x) \log g(x) \phi(x) d x .
\]</div>
<p><a class="reference internal" href="#figure-example2"><span class="std std-numref">Fig. 25.2</span></a> plots the functions <span class="math notranslate nohighlight">\(g \log g\)</span> and <span class="math notranslate nohighlight">\(g -1\)</span>
over the interval <span class="math notranslate nohighlight">\(g \geq 0\)</span>.</p>
<p>That relative entropy <span class="math notranslate nohighlight">\(\textrm{ent}(g) \geq 0\)</span> can be established by noting (a) that  <span class="math notranslate nohighlight">\(g \log g \geq g-1\)</span> (see  <a class="reference internal" href="#figure-example2"><span class="std std-numref">Fig. 25.2</span></a>)
and (b) that under <span class="math notranslate nohighlight">\(\phi\)</span>, <span class="math notranslate nohighlight">\(E g =1\)</span>.</p>
<p><a class="reference internal" href="#figure-example3"><span class="std std-numref">Fig. 25.3</span></a> and <a class="reference internal" href="#figure-example4"><span class="std std-numref">Fig. 25.4</span></a> display aspects of relative entropy visually for a continuous random variable <span class="math notranslate nohighlight">\(x\)</span> for
two densities with likelihood ratio <span class="math notranslate nohighlight">\(g \geq 0\)</span>.</p>
<p>Where the numerator density is <span class="math notranslate nohighlight">\({\mathcal N}(0,1)\)</span>, for two denominator  Gaussian densities <span class="math notranslate nohighlight">\({\mathcal N}(0,1.5)\)</span> and <span class="math notranslate nohighlight">\({\mathcal N}(0,.95)\)</span>, respectively, <a class="reference internal" href="#figure-example3"><span class="std std-numref">Fig. 25.3</span></a> and <a class="reference internal" href="#figure-example4"><span class="std std-numref">Fig. 25.4</span></a>  display the functions  <span class="math notranslate nohighlight">\(g \log g\)</span> and <span class="math notranslate nohighlight">\(g -1\)</span> as functions of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<figure class="align-default" id="figure-example2">
<a class="reference internal image-reference" href="_images/entropy_glogg.png"><img alt="_images/entropy_glogg.png" src="_images/entropy_glogg.png" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 25.2 </span><span class="caption-text">The function <span class="math notranslate nohighlight">\(g \log g\)</span> for <span class="math notranslate nohighlight">\(g \geq 0\)</span>. For a random variable <span class="math notranslate nohighlight">\(g\)</span> with <span class="math notranslate nohighlight">\(E g =1\)</span>, <span class="math notranslate nohighlight">\(E g \log g \geq 0\)</span>.</span><a class="headerlink" href="#figure-example2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="figure-example3">
<a class="reference internal image-reference" href="_images/entropy_1_over_15.jpg"><img alt="_images/entropy_1_over_15.jpg" src="_images/entropy_1_over_15.jpg" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 25.3 </span><span class="caption-text">Graphs of <span class="math notranslate nohighlight">\(g \log g\)</span> and <span class="math notranslate nohighlight">\(g-1\)</span> where  <span class="math notranslate nohighlight">\(g\)</span> is the ratio of the density of a <span class="math notranslate nohighlight">\({\mathcal N}(0,1)\)</span> random variable to the density of a <span class="math notranslate nohighlight">\({\mathcal N}(0,1.5)\)</span> random variable.
Under the <span class="math notranslate nohighlight">\({\mathcal N}(0,1.5)\)</span> density, <span class="math notranslate nohighlight">\(E g =1\)</span>.</span><a class="headerlink" href="#figure-example3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="figure-example4">
<a class="reference internal image-reference" href="_images/entropy_1_over_95.png"><img alt="_images/entropy_1_over_95.png" src="_images/entropy_1_over_95.png" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 25.4 </span><span class="caption-text"><span class="math notranslate nohighlight">\(g \log g\)</span> and <span class="math notranslate nohighlight">\(g-1\)</span> where  <span class="math notranslate nohighlight">\(g\)</span> is the ratio of the density of a <span class="math notranslate nohighlight">\({\mathcal N}(0,1)\)</span> random variable to the density of a <span class="math notranslate nohighlight">\({\mathcal N}(0,1.5)\)</span> random variable.
Under the <span class="math notranslate nohighlight">\({\mathcal N}(0,1.5)\)</span> density, <span class="math notranslate nohighlight">\(E g =1\)</span>.</span><a class="headerlink" href="#figure-example4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                    </div>
                    
                </main> <!-- .page__content -->
                


                <footer class="qe-page__footer">

                    <p><a href="https://creativecommons.org/licenses/by-sa/4.0/"><img src="https://licensebuttons.net/l/by-sa/4.0/80x15.png"></a></p>

                    <p>Creative Commons License &ndash; This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International.</p>

                </footer> <!-- .page__footer -->

            </div> <!-- .page -->

            

            
            <div class="qe-sidebar bd-sidebar inactive" id="site-navigation">

                <div class="qe-sidebar__header">


                    Contents

                </div>

                <nav class="qe-sidebar__nav" id="qe-sidebar-nav" aria-label="Main navigation">
                    <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tools and Techniques
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="orth_proj.html">
   1. Orthogonal Projections and Their Applications
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="stationary_densities.html">
   2. Continuous State Markov Chains
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="muth_kalman.html">
   3. Reverse Engineering a la Muth
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="discrete_dp.html">
   4. Discrete State Dynamic Programming
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  LQ Control
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="cons_news.html">
   5. Information and Consumption Smoothing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="smoothing.html">
   6. Consumption Smoothing with Complete and Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="smoothing_tax.html">
   7. Tax Smoothing with Complete and Incomplete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markov_jump_lq.html">
   8. Markov Jump Linear Quadratic Dynamic Programming
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tax_smoothing_1.html">
   9. How to Pay for a War: Part 1
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tax_smoothing_2.html">
   10. How to Pay for a War: Part 2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tax_smoothing_3.html">
   11. How to Pay for a War: Part 3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lqramsey.html">
   12. Optimal Taxation in an LQ Economy
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Multiple Agent Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="arellano.html">
   13. Default Risk and Income Fluctuations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="matsuyama.html">
   14. Globalization and Cycles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="coase.html">
   15. Coase’s Theory of the Firm
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dynamic Linear Economies
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="hs_recursive_models.html">
   16. Recursive Models of Dynamic Linear Economies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="growth_in_dles.html">
   17. Growth in Dynamic Linear Economies
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lucas_asset_pricing_dles.html">
   18. Lucas Asset Pricing Using DLE
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="irfs_in_hall_model.html">
   19. IRFs in Hall Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="permanent_income_dles.html">
   20. Permanent Income Model using the DLE Class
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rosen_schooling_model.html">
   21. Rosen Schooling Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cattle_cycles.html">
   22. Cattle Cycles
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="hs_invertibility_example.html">
   23. Shock Non Invertibility
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Risk, Model Uncertainty, and Robustness
 </span>
</p>
<ul class="current nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="five_preferences.html">
   24. Risk and Model Uncertainty
  </a>
 </li>
 <li class="toctree-l1 current active active">
  <a class="current reference internal" href="#">
   25. Etymology of Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="robustness.html">
   26. Robustness
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rob_markov_perf.html">
   27. Robust Markov Perfect Equilibrium
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Time Series Models
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="arma.html">
   28. Covariance Stationary Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="estspec.html">
   29. Estimation of Spectra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="additive_functionals.html">
   30. Additive and Multiplicative Functionals
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="lu_tricks.html">
   31. Classical Control with Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classical_filtering.html">
   32. Classical Prediction and Filtering With Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="knowing_forecasts_of_others.html">
   33. Knowing the Forecasts of Others
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Asset Pricing and Finance
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="lucas_model.html">
   34. Asset Pricing II: The Lucas Asset Pricing Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="asset_pricing_lph.html">
   35. Elementary Asset Pricing Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="black_litterman.html">
   36. Two Modifications of Mean-Variance Portfolio Theory
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BCG_complete_mkts.html">
   37. Irrelevance of Capital Structures with Complete Markets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BCG_incomplete_mkts.html">
   38. Equilibrium Capital Structures with Incomplete Markets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming Squared
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="un_insure.html">
   39. Optimal Unemployment Insurance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dyn_stack.html">
   40. Stackelberg Plans
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="calvo.html">
   41. Ramsey Plans, Time Inconsistency, Sustainable Plans
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="opt_tax_recur.html">
   42. Optimal Taxation with State-Contingent Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="amss.html">
   43. Optimal Taxation without State-Contingent Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="amss2.html">
   44. Fluctuating Interest Rates Deliver Fiscal Insurance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="amss3.html">
   45. Fiscal Risk and Government Debt
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chang_ramsey.html">
   46. Competitive Equilibria of a Model of Chang
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chang_credible.html">
   47. Credible Government Policies in a Model of Chang
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Other
 </span>
</p>
<ul class="nav bd-sidenav nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="troubleshooting.html">
   48. Troubleshooting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="zreferences.html">
   49. References
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="status.html">
   50. Execution Statistics
  </a>
 </li>
</ul>

                </nav>

                <div class="qe-sidebar__footer">

                </div>

            </div> <!-- .sidebar -->
            
        </div> <!-- .main -->

        <div class="qe-toolbar">

            <div class="qe-toolbar__inner">

                <ul class="qe-toolbar__main">
                    <li data-tippy-content="Table of Contents" class="btn__sidebar"><i data-feather="menu"></i></li>
                    <li data-tippy-content="Home"><a href="intro.html"><i data-feather="home"></i></a></li>
                    <li class="btn__qelogo"><a href="https://quantecon.org" title=""><span class="show-for-sr">QuantEcon</span></a></li>
                    <!-- <li class="btn__search">
                        <form action="search.html" method="get">
                            <input type="search" class="form-control" name="q" id="search-input" placeholder="Search..." aria-label="Search..." autocomplete="off">
                            <i data-feather="search"></i>
                        </form>
                    </li> -->
                </ul>

                <ul class="qe-toolbar__links">
                    <li data-tippy-content="Fullscreen" class="btn__fullscreen"><i data-feather="maximize"></i></li>
                    <li data-tippy-content="Increase font size" class="btn__plus"><i data-feather="plus-circle"></i></li>
                    <li data-tippy-content="Decrease font size" class="btn__minus"><i data-feather="minus-circle"></i></li>
                    <li data-tippy-content="Change contrast" class="btn__contrast"><i data-feather="sunset"></i></li>
                    <li data-tippy-content="Download Notebook"><a href="/_notebooks/entropy.ipynb" download><i data-feather="download-cloud"></i></a></li>
                    <li class="settings-button" id="settingsButton"><div data-tippy-content="Launch Notebook"><i data-feather="play-circle"></i></div></li>
                        <li class="download-pdf" id="downloadButton"><i data-feather="file"></i></li>
                    <li data-tippy-content="View Source"><a target="_blank" href="https://github.com/QuantEcon/lecture-python-advanced.myst/tree/master/lectures/entropy.md" download><i data-feather="github"></i></a></li>
                </ul>

            </div>

        </div> <!-- .toolbar -->
        <div id="downloadPDFModal" style="display: none;">
            <ul class="pdf-options" style="display: block;">
                <li class="download-pdf-book" onClick="window.print()">
                    <p>Lecture (PDF)</p>
                </li>
                <li class="download-pdf-file">
                    <a href="/_pdf/quantecon-python-advanced.pdf" download><p>Book (PDF)</p></a>
                </li>
            </ul>
        </div>
        <div id="settingsModal" style="display: none;">
            <p class="modal-title"> Notebook Launcher </p>
            <div class="modal-desc">
            <p>
                Choose public or private cloud service for "Launch" button.
            </p>
            </div>
            <p class="modal-subtitle">Select a server</p>
            <ul class="modal-servers">
            <li class="active launcher-public">
                <span class="label">Public</span>
                <select id="launcher-public-input">
                
                    <option value="https://mybinder.org/v2/gh/QuantEcon/lecture-python-advanced.notebooks/master?urlpath=tree/entropy.ipynb">BinderHub</option>
                
                    <option value="https://colab.research.google.com/github/QuantEcon/lecture-python-advanced.notebooks/blob/master/entropy.ipynb">Colab</option>
                
                </select>
                <i class="fas fa-check-circle"></i>
            </li>
            <li class="launcher-private">
                <span class="label">Private</span>
                <input type="text" id="launcher-private-input" data-repourl="https://github.com/QuantEcon/lecture-python-advanced.notebooks" data-urlpath="tree/lecture-python-advanced.notebooks/entropy.ipynb" data-branch=master>
                <i class="fas fa-check-circle"></i>
            </li>
            </ul>
            <p class="launch"><a href="https://mybinder.org/v2/gh/QuantEcon/lecture-python-advanced.notebooks/master?urlpath=tree/entropy.ipynb" id="advancedLaunchButton" target="_blank">Launch Notebook</a></p>
            <script>
                // QuantEcon Notebook Launcher
                const launcherTypeElements = document.querySelectorAll('#settingsModal .modal-servers li');
                // Highlight the server type if previous selection exists
                if (typeof localStorage.launcherType !== 'undefined') {
                  for (var i = 0; i < launcherTypeElements.length; i++) {
                    launcherTypeElements[i].classList.remove('active');
                    if ( launcherTypeElements[i].classList.contains(localStorage.launcherType) ) {
                      launcherTypeElements[i].classList.add('active');
                    }
                  }
                }
                // Highlight server type on click and set local storage value
                for (var i = 0; i < launcherTypeElements.length; i++) {
                  launcherTypeElements[i].addEventListener('click', function() {
                    for (var j = 0; j < launcherTypeElements.length; j++) {
                      launcherTypeElements[j].classList.remove('active');
                    }
                    this.classList.add('active');
                    if ( this.classList.contains('launcher-private') ) {
                      localStorage.launcherType = 'launcher-private';
                    } else if ( this.classList.contains('launcher-public') ) {
                      localStorage.launcherType = 'launcher-public';
                    }
                    setLaunchServer();
                  })
                }
                const launcherPublic = document.getElementById('launcher-public-input');
                const launcherPrivate = document.getElementById('launcher-private-input');
                const pageName = "entropy";
                const repoURL = "https://github.com/QuantEcon/lecture-python-advanced.notebooks";
                const urlPath = "tree/lecture-python-advanced.notebooks/entropy.ipynb";
                const branch = "master"
                const launchNotebookLink = document.getElementById('advancedLaunchButton');

                // Highlight public server option if previous selection exists
                if (typeof localStorage.launcherPublic !== 'undefined') {
                  launcherPublic.value = localStorage.launcherPublic;
                }
                // Update local storage upon public server selection
                launcherPublic.addEventListener('change', (event) => {
                  setLaunchServer();
                });
                // Populate private server input if previous entry exists
                if (typeof localStorage.launcherPrivate !== 'undefined') {
                  launcherPrivate.value = localStorage.launcherPrivate;
                }
                // Update local storage when a private server is entered
                launcherPrivate.addEventListener('input', (event) => {
                  setLaunchServer();
                });

                // Function to update the "Launch Notebook" link href
                function setLaunchServer() {
                  launchNotebookLink.removeAttribute("style")
                  if ( localStorage.launcherType == 'launcher-private' ) {
                    let repoPrefix = "/jupyter/hub/user-redirect/git-pull?repo=" + repoURL + "&branch=" + branch + "&urlpath=" + urlPath;
                    launcherPrivateValue = launcherPrivate.value
                    if (!launcherPrivateValue) {
                        launchNotebookLink.removeAttribute("href")
                        launchNotebookLink.style.background = "grey"
                        return
                    }
                    localStorage.launcherPrivate = launcherPrivateValue;
                    privateServer = localStorage.launcherPrivate.replace(/\/$/, "")
                    if (!privateServer.includes("http")) {
                        privateServer = "http://" + privateServer
                    }
                    launchNotebookLinkURL = privateServer + repoPrefix;
                  } else if ( localStorage.launcherType == 'launcher-public' ) {
                    launcherPublicValue = launcherPublic.options[launcherPublic.selectedIndex].value;
                    localStorage.launcherPublic = launcherPublicValue;
                    launchNotebookLinkURL = localStorage.launcherPublic;
                  }
                  if (launchNotebookLinkURL) launchNotebookLink.href = launchNotebookLinkURL;
                }
                // Check if user has previously selected a server
                if ( (typeof localStorage.launcherPrivate !== 'undefined') || (typeof localStorage.launcherPublic !== 'undefined') ) {
                  setLaunchServer();
                }
                </script>

        </div>

    </div> <!-- .wrapper-->
  </body>
</html>